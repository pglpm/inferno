<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>A crash course in population inference • inferno</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="A crash course in population inference">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">inferno</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/vignette_crashcourse.html">A crash course in population inference</a></li>
    <li><a class="dropdown-item" href="../articles/vignette_mutualinfo.html">Associations of variates with mutual information</a></li>
    <li><a class="dropdown-item" href="../articles/vignette_parkinson.html">Example of use: inferences for Parkinson's Disease</a></li>
    <li><a class="dropdown-item" href="../articles/vignette_start.html">Example of use: inferno for penguins</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/pglpm/inferno/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>A crash course in population inference</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/pglpm/inferno/blob/main/vignettes/vignette_crashcourse.Rmd" class="external-link"><code>vignettes/vignette_crashcourse.Rmd</code></a></small>
      <div class="d-none name"><code>vignette_crashcourse.Rmd</code></div>
    </div>

    
    
<p>This vignette gives a summary of what
<strong><em>Inferno</em></strong> is about, and the kind of inferences
it allows you to do; a sort of crash course in population inference,
also called exchangeable inference. Its main purpose is to clarify, by
means of examples, the terminology used in this package.</p>
<p>It is of course impossible to summarize this branch of probability
theory and of statistics in a couple of sections; you’re invited to
learn more for instance from the <a href="https://pglpm.github.io/ADA511" class="external-link">ADA 511</a> course, or from texts
such as Bernardo &amp; Smith’s <a href="https://doi.org/10.1002/9780470316870" class="external-link"><em>Bayesian Theory</em>
(2000)</a> (especially §§ 4.3, 4.4, 4.6), or Jaynes’s <a href="https://doi.org/10.1017/CBO9780511790423" class="external-link"><em>Probability
Theory</em> (2003)</a> (esp. ch. 9); take also a look at Lindley &amp;
Novick <a href="https://doi.org/10.1214/aos/1176345331" class="external-link"><em>The role of
exchangeability in inference</em> (1981)</a>.</p>
<div class="section level2">
<h2 id="population-inferences">Population inferences<a class="anchor" aria-label="anchor" href="#population-inferences"></a>
</h2>
<p>A very common type of inference is the following. We have a large or
potentially infinite group of entities, each having a set of
characteristics. Having checked the characteristics of several entities
from this group, we want to make guesses about the characteristics of
new entities from the same group.</p>
<p>The description just given is very generic or even vague. Indeed it
can be applied to a huge variety of situations.</p>
<ul>
<li>The ‘entities’ could be: objects, like electronic components; or
people, or animals, or plants, or events of some kind.</li>
<li>The characteristics could be: the material the object is made of,
and the result of some test made on it; or the age, blood-test results,
and disease condition of the person; or the species and body mass of the
animal; or the length of the petals of the flower.</li>
<li>The group could be: electronic components coming out of a particular
production line; or people of a given nationality and with given
symptoms; or animals from a particular island; or flowers of a specific
genus or family.</li>
</ul>
<p>The possibilities are endless.</p>
<p>Also the kinds of guesses that we want to make can be very diverse.
We might want to guess all characteristics of a new entity; or, having
checked <em>some</em> characteristics of a new entity, we want to guess
the ones that we haven’t or can’t check. Important examples of this kind
of inference appear in medicine. For example we may have a group defined
as follows:</p>
<ul>
<li>
<em>Group</em>: people from a given nationality, suffering from one
of two possible diseases.</li>
</ul>
<p>And we may consider these characteristics:</p>
<ul>
<li>
<em>Characteristics</em>: age; sex; weight; presence or absence of a
particular genetic factor; symptoms from a specific set of possible
ones; results from clinical tests taken at different times; kind of
disease.</li>
</ul>
<p>We observed as many as possible of these characteristics in a sample
of people from this group. Now a new person from the same group appears
in front of us. We check this person’s age, sex, weight, symptoms. We
need to guess which of the two diseases affects this person.</p>
<p><br>
The kind of inference summarily described above has one important
aspect. Suppose you have collected a sample from your group of interest,
and you want to use this sample to make guesses about new entities from
the same group. If someone exchanged one entity in your sample with
another one <em>unsystematically</em> chosen from the group, then you
wouldn’t protest. After all, you still have the same number of samples
from the same group. This aspect is called
<strong>exchangeability</strong>: we say that this kind of inference is
exchangeable. There are kinds of inference for which this aspect is not
true. For example, suppose you’re given some stock data from four
consecutive days, and you want to make guesses about the next day. If
someone replaced any of your four datapoints with a datapoint an
unsystematically chosen other day of the year, then you would protest:
the time order of the datapoints matter. This is an example of
non-exchangeable inference.</p>
<p>The inferences for which <em>inferno</em> is designed are
exchangeable ones. We shall also call them <strong>population
inferences</strong>.</p>
<p><br>
Having discussed these simple examples, let’s agree on some more
standard terminology. Let’s call:</p>
<ul>
<li>
<strong>Population</strong>: what we’ve called ‘group’.</li>
<li>
<strong>Unit</strong>: what we’ve called ‘entity’ – the object,
person, animal, etc.</li>
<li>
<strong>Variate</strong>: what we’ve called ‘characteristic’.</li>
</ul>
</div>
<div class="section level2">
<h2 id="probability-and-population-frequencies">Probability and population frequencies<a class="anchor" aria-label="anchor" href="#probability-and-population-frequencies"></a>
</h2>
<p>We have been speaking about “making guesses”; but what does this
mean?</p>
<p>When a unit is chosen unsystematically from a population, it’s only
in rare situations that we may be sure about the variates of this unit
before checking them. Suppose for instance that the population of
interest is ‘all adults from a given country’; and the variates of
interest are <code>sex</code>, <code>weight</code>, <code>height</code>.
If a person is chosen from this population, we can’t be sure beforehand
of how tall this person will be. We can exclude values like 4 m or
20 cm, but we’ll be uncertain about many other possible values. Even if
someone tells us the sex and weight of this person, we’ll still be in
doubt, but maybe we can consider some values more probable than
others.</p>
<p>That’s the keyword: <strong>probability</strong>. Although we are
unsure about a variate of a unit, we can still find some values more
probable than others: we may consider it more probable that the person
is 160 cm tall than 180 cm tall; or in a clinical inference we may
consider it more probable that the patient has a particular virus than
not.</p>
<p>Probability is our <em>degree of belief</em> about the value of the
unknown variate.</p>
<p>Note that probability is not a physical property of the unit. For
instance, the person in question may be exactly 158 cm tall; the
probability of being 180 cm tall is not something we can “measure” from
the person. Also, if we found out some other variate of the person – say
we knew the weight, and now we know also the sex – then the probability
we assign to 180 cm may increase or decrease; but the person is exactly
the same as before. Probability is not a property of the population
either. For instance, for one person from a population we may think
150 cm to be the most probable height, but for another person from the
same population we may think 180 cm to be the most probable height
instead.</p>
<p>Probability expresses the <em>information</em> we have about the
variate of a unit. This is why another researcher may have a different
probability about the same unit: because they may have different
information about that unit.</p>
<p><br>
There is a situation in which we would all agree about the probability
about a variate of a unit: when we know the <strong>frequencies</strong>
for that variate in the population. Suppose for instance that the
population of interest is that of penguins who lived in particular
locations in some particular years (penguins from other locations or
other times are not part of this population). The variates are the
penguins’ <code>species</code> and the <code>island</code> they lived
in.</p>
<p>You’re told that, as a matter of fact, 43.7% of penguins from the
<em>whole</em> population are of species <em>Adelie</em>, 20.0% of
species <em>Chinstrap</em>, and 36.3% of species <em>Gentoo</em>. Now a
penguin from that population is brought in front of you, but you can’t
see any of its characteristics. What’s your degree of belief that this
penguin is of species <em>Adelie</em>, or <em>Chinstrap</em>, or
<em>Gentoo</em>? We’d all agree, given the frequency information about
this population, to assign the probabilities of 43.7%, 20.0%, and 36.3%
to the three possibilities, for this penguin. We write this as follows:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="monospace">𝚜𝚙𝚎𝚌𝚒𝚎𝚜</mtext><mspace width="-0.167em"></mspace><mo>=</mo><mspace width="-0.167em"></mspace><mrow><mi>𝐴</mi><mi>𝑑</mi><mi>𝑒</mi><mi>𝑙</mi><mi>𝑖</mi><mi>𝑒</mi></mrow><mo>∣</mo><mtext mathvariant="sans-serif">𝗉𝗈𝗉𝗎𝗅𝖺𝗍𝗂𝗈𝗇 𝖿𝗋𝖾𝗊𝗎𝖾𝗇𝖼𝗒</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0.437</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="monospace">𝚜𝚙𝚎𝚌𝚒𝚎𝚜</mtext><mspace width="-0.167em"></mspace><mo>=</mo><mspace width="-0.167em"></mspace><mrow><mi>𝐶</mi><mi mathvariant="italic">h</mi><mi>𝑖</mi><mi>𝑛</mi><mi>𝑠</mi><mi>𝑡</mi><mi>𝑟</mi><mi>𝑎</mi><mi>𝑝</mi></mrow><mo>∣</mo><mtext mathvariant="sans-serif">𝗉𝗈𝗉𝗎𝗅𝖺𝗍𝗂𝗈𝗇 𝖿𝗋𝖾𝗊𝗎𝖾𝗇𝖼𝗒</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0.200</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="monospace">𝚜𝚙𝚎𝚌𝚒𝚎𝚜</mtext><mspace width="-0.167em"></mspace><mo>=</mo><mspace width="-0.167em"></mspace><mrow><mi>𝐺</mi><mi>𝑒</mi><mi>𝑛</mi><mi>𝑡</mi><mi>𝑜</mi><mi>𝑜</mi></mrow><mo>∣</mo><mtext mathvariant="sans-serif">𝗉𝗈𝗉𝗎𝗅𝖺𝗍𝗂𝗈𝗇 𝖿𝗋𝖾𝗊𝗎𝖾𝗇𝖼𝗒</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0.363</mn></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
&amp;\mathrm{Pr}(\texttt{species}\!=\!\mathit{Adelie} \mid 
\textsf{population frequency}) = 0.437
\\
&amp;\mathrm{Pr}(\texttt{species}\!=\!\mathit{Chinstrap} \mid 
\textsf{population frequency}) = 0.200
\\
&amp;\mathrm{Pr}(\texttt{species}\!=\!\mathit{Gentoo} \mid 
\textsf{population frequency}) = 0.363
\end{aligned}
</annotation></semantics></math> On the left side of the
‘<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>∣</mo><annotation encoding="application/x-tex">\mid</annotation></semantics></math>’
bar we write what we’re guessing or are unsure about. On the right side,
we write the information that led to our probability assignment; in this
case, the information about the full population. When the information
behind a probability is understood, the
‘<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>∣</mo><annotation encoding="application/x-tex">\mid</annotation></semantics></math>’
bar and the right side are usually omitted.</p>
<p>In this case, the highest probability is for the <em>Adelie</em>
species, but the other two possibilities cannot be excluded. The
numerical values of these probabilities are extremely important, because
they determine any kind of <strong>decision</strong> we may have to make
about our unit. This is especially true in medical decision-making,
where probabilities, combined with <strong>utilities</strong>, determine
which is the best choice that a clinician can make. Medical and clinical
decision-making, and the role of probabilities in them, are discussed
for instance in the texts by Sox &amp; al.: <a href="https://doi.org/10.1002/9781119627876" class="external-link"><em>Medical Decision
Making</em> (2024)</a>, by Hunink &amp; al.: <a href="https://doi.org/10.1017/CBO9781139506779" class="external-link"><em>Decision Making in
Health and Medicine</em> (2014)</a>, or by Weinstein &amp; Fineberg <a href="https://archive.org/details/clinicaldecision0000unse_e3n8" class="external-link"><em>Clinical
Decision Analysis</em> (1980)</a>.</p>
<p>The example with the penguin population can be analysed further. The
penguins in this population come from three possible
<code>island</code>s: <em>Biscoe</em>, <em>Dream</em>, and
<em>Torgersen</em>. Therefore a penguin can be of one of three species
and from one of three islands, for a total of 3 × 3 = 9 possibilities.
You’re told that, as a matter of fact, the frequencies of these nine
combinations in the whole population are as follows:</p>
<table class="table"><tbody>
<tr class="odd">
<td align="center"></td>
<td align="center"><em>Adelie</em></td>
<td align="center"><em>Chinstrap</em></td>
<td align="center"><em>Gentoo</em></td>
</tr>
<tr class="even">
<td align="center"><em>Biscoe</em></td>
<td align="center">12.6%</td>
<td align="center">1.8%</td>
<td align="center">34.0%</td>
</tr>
<tr class="odd">
<td align="center"><em>Dream</em></td>
<td align="center">17.4%</td>
<td align="center">16.5%</td>
<td align="center">1.4%</td>
</tr>
<tr class="even">
<td align="center"><em>Torgersen</em></td>
<td align="center">13.8%</td>
<td align="center">1.6%</td>
<td align="center">0.9%</td>
</tr>
</tbody></table>
<p>Then we’d all agree that the probability that the penguin brought to
us is of <code>species</code> <em>Gentoo</em> and from
<code>island</code> <em>Biscoe</em> would be 34.0%:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="monospace">𝚜𝚙𝚎𝚌𝚒𝚎𝚜</mtext><mspace width="-0.167em"></mspace><mo>=</mo><mspace width="-0.167em"></mspace><mrow><mi>𝐺</mi><mi>𝑒</mi><mi>𝑛</mi><mi>𝑡</mi><mi>𝑜</mi><mi>𝑜</mi></mrow><mo>,</mo><mtext mathvariant="monospace">𝚒𝚜𝚕𝚊𝚗𝚍</mtext><mspace width="-0.167em"></mspace><mo>=</mo><mspace width="-0.167em"></mspace><mrow><mi>𝐵</mi><mi>𝑖</mi><mi>𝑠</mi><mi>𝑐</mi><mi>𝑜</mi><mi>𝑒</mi></mrow><mo>∣</mo><mtext mathvariant="sans-serif">𝗉𝗈𝗉𝗎𝗅𝖺𝗍𝗂𝗈𝗇 𝖿𝗋𝖾𝗊𝗎𝖾𝗇𝖼𝗒</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0.340</mn></mrow><annotation encoding="application/x-tex">
\mathrm{Pr}(
\texttt{species}\!=\!\mathit{Gentoo} ,
\texttt{island}\!=\!\mathit{Biscoe} 
\mid 
\textsf{population frequency}) = 0.340
</annotation></semantics></math></p>
<p><br><strong><em>Inferno</em></strong> allows you to calculate probabilities
of this kind, for any set of variates of your choice.</p>
</div>
<div class="section level2">
<h2 id="learning-from-known-variates">Learning from known variates<a class="anchor" aria-label="anchor" href="#learning-from-known-variates"></a>
</h2>
<p>But there’s even more interesting information in the population
frequencies above. Let’s focus on <em>Biscoe</em> island. With a quick
sum we see that 48.4% of the whole population comes from <em>Biscoe</em>
island (thus we’d assign a probability of 0.484 that our penguin comes
from that island). Dividing the frequencies above, row-wise, by the
frequencies of the respective islands (the sums of each row), we can
find the frequency of each species <em>for each particular
island</em>:</p>
<table class="table"><tbody>
<tr class="odd">
<td align="center"></td>
<td align="center"><em>Adelie</em></td>
<td align="center"><em>Chinstrap</em></td>
<td align="center"><em>Gentoo</em></td>
</tr>
<tr class="even">
<td align="center">from <em>Biscoe</em>
</td>
<td align="center">26.03%</td>
<td align="center">3.72%</td>
<td align="center">70.25%</td>
</tr>
<tr class="odd">
<td align="center">from <em>Dream</em>
</td>
<td align="center">49.29%</td>
<td align="center">46.74%</td>
<td align="center">3.97%</td>
</tr>
<tr class="even">
<td align="center">from <em>Torgersen</em>
</td>
<td align="center">84.66%</td>
<td align="center">9.82%</td>
<td align="center">5.52%</td>
</tr>
</tbody></table>
<p>We call these <strong>conditional frequencies</strong>, and we call
the group of penguins that come from <code>island</code>
<em>Biscoe</em>a <strong>subpopulation</strong> of the whole population.
The table above reports the frequencies of the three
<code>species</code> in each subpopulation.</p>
<p>Thus we also know, for instance, that 70.25% of penguins in the
subpopulation from <code>island</code> <em>Biscoe</em> are of
<code>species</code> <em>Gentoo</em>. This species is the majority in
that subpopulation – contrast this with the majority in the whole
population, which we saw was <em>Adelie</em>. Indeed, your most probable
guess about the <code>species</code> of the penguin in front of you was
<em>Adelie</em>, with 0.437 probability.</p>
<p>But suppose now someone tells you that this penguin comes from
<code>island</code> <em>Biscoe</em> (so this variate is now known to
you). Given this new piece of information, which probabilities do you
assign to the <code>species</code> of this penguin? Obviously 0.2603 for
<em>Adelie</em>, 0.0372 for <em>Chinstrap</em>, and 0.7025 for
<em>Gentoo</em>. We write this as follows:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="monospace">𝚜𝚙𝚎𝚌𝚒𝚎𝚜</mtext><mspace width="-0.167em"></mspace><mo>=</mo><mspace width="-0.167em"></mspace><mrow><mi>𝐴</mi><mi>𝑑</mi><mi>𝑒</mi><mi>𝑙</mi><mi>𝑖</mi><mi>𝑒</mi></mrow><mo>∣</mo><mtext mathvariant="monospace">𝚒𝚜𝚕𝚊𝚗𝚍</mtext><mspace width="-0.167em"></mspace><mo>=</mo><mspace width="-0.167em"></mspace><mrow><mi>𝐵</mi><mi>𝑖</mi><mi>𝑠</mi><mi>𝑐</mi><mi>𝑜</mi><mi>𝑒</mi></mrow><mo>,</mo><mtext mathvariant="sans-serif">𝗉𝗈𝗉𝗎𝗅𝖺𝗍𝗂𝗈𝗇 𝖿𝗋𝖾𝗊𝗎𝖾𝗇𝖼𝗒</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0.2603</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="monospace">𝚜𝚙𝚎𝚌𝚒𝚎𝚜</mtext><mspace width="-0.167em"></mspace><mo>=</mo><mspace width="-0.167em"></mspace><mrow><mi>𝐶</mi><mi mathvariant="italic">h</mi><mi>𝑖</mi><mi>𝑛</mi><mi>𝑠</mi><mi>𝑡</mi><mi>𝑟</mi><mi>𝑎</mi><mi>𝑝</mi></mrow><mo>∣</mo><mtext mathvariant="monospace">𝚒𝚜𝚕𝚊𝚗𝚍</mtext><mspace width="-0.167em"></mspace><mo>=</mo><mspace width="-0.167em"></mspace><mrow><mi>𝐵</mi><mi>𝑖</mi><mi>𝑠</mi><mi>𝑐</mi><mi>𝑜</mi><mi>𝑒</mi></mrow><mo>,</mo><mtext mathvariant="sans-serif">𝗉𝗈𝗉𝗎𝗅𝖺𝗍𝗂𝗈𝗇 𝖿𝗋𝖾𝗊𝗎𝖾𝗇𝖼𝗒</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0.0372</mn></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="left" style="text-align: left"><mrow><mi mathvariant="normal">P</mi><mi mathvariant="normal">r</mi></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="monospace">𝚜𝚙𝚎𝚌𝚒𝚎𝚜</mtext><mspace width="-0.167em"></mspace><mo>=</mo><mspace width="-0.167em"></mspace><mrow><mi>𝐺</mi><mi>𝑒</mi><mi>𝑛</mi><mi>𝑡</mi><mi>𝑜</mi><mi>𝑜</mi></mrow><mo>∣</mo><mtext mathvariant="monospace">𝚒𝚜𝚕𝚊𝚗𝚍</mtext><mspace width="-0.167em"></mspace><mo>=</mo><mspace width="-0.167em"></mspace><mrow><mi>𝐵</mi><mi>𝑖</mi><mi>𝑠</mi><mi>𝑐</mi><mi>𝑜</mi><mi>𝑒</mi></mrow><mo>,</mo><mtext mathvariant="sans-serif">𝗉𝗈𝗉𝗎𝗅𝖺𝗍𝗂𝗈𝗇 𝖿𝗋𝖾𝗊𝗎𝖾𝗇𝖼𝗒</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0.7025</mn></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{aligned}
&amp;\mathrm{Pr}(\texttt{species}\!=\!\mathit{Adelie} \mid 
\texttt{island}\!=\!\mathit{Biscoe} ,
\textsf{population frequency}) = 0.2603
\\
&amp;\mathrm{Pr}(\texttt{species}\!=\!\mathit{Chinstrap} \mid 
\texttt{island}\!=\!\mathit{Biscoe} ,
\textsf{population frequency}) = 0.0372
\\
&amp;\mathrm{Pr}(\texttt{species}\!=\!\mathit{Gentoo} \mid 
\texttt{island}\!=\!\mathit{Biscoe} ,
\textsf{population frequency}) = 0.7025
\end{aligned}
</annotation></semantics></math> The right side of the
‘<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mo>∣</mo><annotation encoding="application/x-tex">\mid</annotation></semantics></math>’
bar now reports the extra information that the penguin comes from
<code>island</code> <em>Biscoe</em>. We say that our probability has
been <em>updated</em>, and we call it a <strong>conditional
probability</strong>.</p>
<p>Learning about the penguin’s <code>island</code> not only made you
change the highest probability assignment from <em>Aelie</em> to
<em>Gentoo</em>, but it also increased the value of the highest
probability. Without knowing the penguin’s <code>island</code>,
<em>Adelie</em> had slightly less than 50% probability; it was as likely
as not. After learning the <code>island</code> variate, <em>Gentoo</em>
gets more than 70% probability.</p>
<p>An analogous discussion can be made for a continuous variate. Let’s
for instance take the <code>body_mass</code> of the penguins,
discretized in steps of 0.025 kg. Suppose we knew that the histogram of
body masses in the whole population were as follows:</p>
<p><img src="prob-body_mass.jpg" width="75%" style="display: block; margin: auto;"></p>
<p>Then, if you had to guess the body mass of the penguin brought to
you, you’d give the value 4 kg (plus or minus 0.0125 kg) a probability
of around 1%, and so on for other possible values; the most probable
value being 3.6 kg at 1.4% probability.</p>
<p>Suppose you’re now given also the histograms for the subpopulations
from the three islands:</p>
<p><img src="prob-body_mass-given_island.jpg" width="75%" style="display: block; margin: auto;"></p>
<p>Upon learning that your penguin is from <em>Biscoe</em>, your degree
of belief would change: the most probable value would be 4.8 kg at 1.3%
probability. Note how the histograms for <code>body_mass</code> are
different in the subpopulations of the three islands. In
<em>Torgersen</em>, for instance, it’s more probable to find penguins
between 3 kg and 4 kg than between 5 kg and 6 kg; whereas in
<em>Biscoe</em> the opposite is true.</p>
<p>This is exactly the kind of learning situation that takes place in
medicine and in clinical inferences. A clinician searches for symptoms
because they may change and increase the probabilities of different
diseases or health conditions. The reason why clinicians research
particular <em>subpopulations</em> – particular demographics, or genetic
factors, etc. – is that within these subgroups the probability that a
medical condition exist or will occur can be drastically higher. In
turn, this leads the clinicians to understand better what can be the
biological relationships between the health condition and those
factors.</p>
<p><br><strong><em>Inferno</em></strong> allows you to calculate the updated
probabilities of any set of variates, conditional on any other set.</p>
</div>
<div class="section level2">
<h2 id="uncertainty-about-whole-populations--population-samples">Uncertainty about whole populations. Population samples<a class="anchor" aria-label="anchor" href="#uncertainty-about-whole-populations--population-samples"></a>
</h2>
<p>Inferences are therefore quite clear if we know the frequencies of
the variates for the <em>whole</em> population. Our main problem is that
we usually <em>don’t know those whole-population frequencies</em>. For
example, do you know the <em>exact</em> number of people, among those
born in your country at any time and are alive today, who are exactly
between 165 cm and 170 cm tall today?</p>
<p>For our penguin example, let’s consider again the
<code>body_mass</code> variate. We actually don’t know what the
histogram of this variate over the whole population looks like. It could
have one or several peaks, a symmetric or non-symmetric shape,
shoulders, an so on. Here are seven possibilities:</p>
<p><img src="penguin_priors.jpg" width="75%" style="display: block; margin: auto;"></p>
<p><br>
It’s often impractical or impossible to measure the frequencies of a
variate in a whole population. We must therefore <em>guess</em>
them.</p>
<p>In order to guess them, we usually examine a <strong>sample</strong>
from the population of interest. This sample is chosen in an
unsystematic way, so that its statistics and its <strong>sample
frequencies</strong> are not affected by peculiar choices, otherwise we
would have a <em>biased</em> sample (imagine for instance choosing a
sample of <em>male</em> penguins only, whereas the penguin population
we’re interested in has both males and females).</p>
<p>But the frequencies of variates in a sample from a population are
typically different from those in the whole population. The difference
is the larger, the smaller the sample. For example, if we chose five
penguins from our penguin population in an unsystematic way, it could
happen that we got five penguins all of species <em>Gentoo</em>. Then
the variate <code>species</code> would have frequencies of 100% for
<em>Gentoo</em> and 0% for <em>Adelie</em> and <em>Chinstrap</em>. But
of course this would not mean that only the species <em>Gentoo</em>
occurs in the whole population. In order to reflect the frequencies in
the whole population, the size of a sample needs to be larger, the
larger are:</p>
<ul>
<li>the numbers of variates considered;</li>
<li>the possible values of each variate.</li>
</ul>
<p>Therefore, to guess the frequencies in the whole population we can
examine a sample from it – but we cannot fully rely on the sample.</p>
<p>The traditional way of facing this uncertainty was to <em>assume</em>
that the whole population has frequencies with particular features; most
commonly, they were assumed to be <em>Gaussian</em>. The variate values
observed in the sample were used to fit the free parameters of the
particular assumed distribution, like the means, variances, covariances
of the Gaussian.</p>
<p>Consider how drastic such an approach is. It can also be misleading,
if one is not aware of what is being done. For instance, errors can be
reported about the fit of the free parameters, and if they are small
they can give a false sense of precise inference. But the very
<em>assumption</em> has errors, which may be quite large; and such
errors are rarely reported.</p>
<p>One could object that the assumption, say of gaussianity, behind such
methods is not completely arbitrary: one can get the idea of whether
it’s correct or not by looking at the sample. But this objection is
self-contradictory: as discussed, our problem is that the frequencies in
the sample are not a faithful reflection of those in the whole
population. We’re making assumption because the sample is not reliable,
so how can we rely on the sample to judge those assumptions?</p>
<p>It must be pointed out that even methods or measures that do not
explicitly refer to peculiar assumptions about frequencies, <em>do</em>
actually rely on such hidden assumptions. Pearson’s measure of
correlation, for instance, actually relies on the assumption that the
joint frequency distribution for the whole population is Gaussian.
<!-- add ref --></p>
</div>
<div class="section level2">
<h2 id="bayesian-nonparametric-population-inference">Bayesian nonparametric population inference<a class="anchor" aria-label="anchor" href="#bayesian-nonparametric-population-inference"></a>
</h2>
<p>The method based on the assumption of peculiar frequency distribution
was a necessity in the past, because there was no other computationally
feasible way of facing the uncertainty about the whole population. But
today the situation is different.</p>
<p>Instead of making peculiar assumptions out of computational
desperation, today we can explicitly recognize our uncertainty about the
frequencies in the whole population, and therefore account for the
errors that can come from this uncertainty. This is what Bayesian
nonparametric population inference does. The term ‘nonparametric’ means
that no peculiar assumptions are made about frequency distribution
having simple shapes (and therefore expressible with few
parameters).</p>
<p>Since we must <em>guess</em> what the frequencies are in the whole
population, we proceed by assigning <em>probabilities</em> to all
possible such frequency distributions. Intuitively, a candidate
frequency distribution is more probable:</p>
<ul>
<li>the more it fits the frequencies measured in the sample;</li>
<li>the more it looks “natural”.</li>
</ul>
<p>An example of unnatural or strange probability distribution, for our
<code>body_mass</code> variate, could be the following:</p>
<p><img src="penguin_prior_strange.jpg" width="75%" style="display: block; margin: auto;"></p>
<p>It seems quite unnatural based on our experience with histograms over
large natural populations. If a sample from a penguin population showed
this frequency distribution of <code>body_mass</code> values, you would
probably check whether any errors were made during the sampling.</p>
<p>The core of the <strong><em>inferno</em></strong> package is the
calculation of the probabilities for the possible frequencies of the
whole population, and the selection of a large sample of the most
probable ones. This calculation and sampling are realized by the
<code><a href="../reference/learn.html">learn()</a></code> function.</p>
</div>
<div class="section level2">
<h2 id="back-to-guessing-about-the-next-unit">Back to guessing about the next unit<a class="anchor" aria-label="anchor" href="#back-to-guessing-about-the-next-unit"></a>
</h2>
<p>There are situations in which it is not possible to choose the sample
in an unsystematic way. Yet such a biased sample can still be used for
population inference, as long as we know what its bias may be.
<strong><em>Inferno</em></strong> allows you to make this kind of bias
corrections as well.</p>
<p>But the frequencies of variates in a sample from a population are
typically different from those in the whole population. The difference
is the larger, the smaller the sample. There are even extreme situations
in which some statistics from a sample, like the mean, can be way off
those of the whole population, even if the sample is large (the
statistics become reliable only when the sample is almost as large as
the full population). The plot below shows an example. The variate is a
real number that can take on positive or negative values, in a
population of 100 000 000 units; the mean value of the variate in the
whole population is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>1.14</mn></mrow><annotation encoding="application/x-tex">-1.14</annotation></semantics></math>.
The plot shows the values of the mean obtained from unsystematically
chosen samples of various sizes. Even in a sample of 30 000 000 units we
may observe a mean of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">-3</annotation></semantics></math>.
The <em>median</em> of a sample is much more reliable.</p>
<p><img src="sample_mean.jpg" width="67%" style="display: block; margin: auto;"></p>
<p>The point of this example is only to make clear that although a
sample from a population gives us some information about the whole
population, we are still uncertain about the whole-population
frequencies.</p>
<!-- ![](sample_mean.jpg){#id .class width=50%} -->
<p><em>(TO BE CONTINUED)</em></p>
<!-- 

----

(OLD TEXT)



We store the name of the datafile

``` r
dataset <- penguins
```
Here are the values for two subjects:


|    |species   |island    | bill_len| bill_dep| flipper_len| body_mass|sex    | year|
|:---|:---------|:---------|--------:|--------:|-----------:|---------:|:------|----:|
|225 |Gentoo    |Biscoe    |     48.2|     15.6|         221|      5100|male   | 2008|
|127 |Adelie    |Torgersen |     38.8|     17.6|         191|      3275|female | 2009|
|315 |Chinstrap |Dream     |     46.9|     16.6|         192|      2700|female | 2008|
|189 |Gentoo    |Biscoe    |     42.6|     13.7|         213|      4950|female | 2008|
|271 |Gentoo    |Biscoe    |     47.2|     13.7|         214|      4925|female | 2009|



### "Natural" vs controlled or "biased" variates

## Metadata

Let's load the package:

``` r
library(inferno)
```


```{.r .fold-hide}
## metadatatemplate(data = datafile, file = 'temp_metadata')
```


``` r
metadatafile <- 'meta_toydata.csv'
```
The metadata are as follows; `NA` indicate empty fields:


|name                                    |type       | datastep| domainmin| domainmax|minincluded |maxincluded |V1     |V2      |
|:---------------------------------------|:----------|--------:|---------:|---------:|:-----------|:-----------|:------|:-------|
|TreatmentGroup                          |nominal    |         |          |          |            |            |NR     |Placebo |
|Sex                                     |nominal    |         |          |          |            |            |Female |Male    |
|Age                                     |continuous |        1|         0|          |            |            |       |        |
|Anamnestic.Loss.of.smell                |nominal    |         |          |          |            |            |No     |Yes     |
|History.of.REM.Sleep.Behaviour.Disorder |nominal    |         |          |          |            |            |No     |Yes     |
|MDS.UPDRS..Subsection.III.V1            |ordinal    |        1|         0|       132|yes         |yes         |       |        |
|diff.MDS.UPRS.III                       |ordinal    |        1|      -132|       132|yes         |yes         |       |        |



*(More to be written here!)*


## Learning

$$
\mathrm{Pr}(Y = y \:\vert\: X = x, \mathrm{data})
$$
As the notation above indicates, *these probabilities also depend on the $\mathrm{data}$ already observed*. They are usually called "posterior probabilities".

We need to prepare the software to perform calculations of posterior probabilities given the observed data. In machine learning an analogous process is called "learning". For this reason the function that achieves this is called `learn()`. It requires at least three arguments:

- `data`, which can be given as a path to the `csv` file containing the data
- `metadata`, which can also be given as a path to the metadata file
- `outputdir`: the name of the directory where the output should be saved.

It may be useful to specify two more arguments:

- `seed`: the seed for the random-number generator, to ensure reproducibility
- `parallel`: the number of CPUs to use for the computation

Alternatively you can set the seed with `set.seed()`, and start a set of parallel workers with the `parallel::makeCluster()` and `doParallel::registerDoParallel()` commands.

The "learning" computation can take tens of minutes, or hours, or even days depending on the number of variates and data in your inference problem. The `learn()` function outputs various messages on how the computation is proceeding. As an example:


``` r
learnt <- learn(
    data = datafile,
    metadata = metadatafile,
    outputdir = 'parkinson_computations',
    seed = 16,
    parallel = 12)

#> Registered doParallelSNOW with 12 workers
#>
#> Using 30 datapoints
#> Calculating auxiliary metadata
#>
#> **************************
#> Saving output in directory
#> parkinson_computations
#> **************************
#> Starting Monte Carlo sampling of 3600 samples by 60 chains
#> in a space of 703 (effectively 6657) dimensions.
#> Using 12 cores: 60 samples per chain, 5 chains per core.
#> Core logs are being saved in individual files.
#>
#> C-compiling samplers appropriate to the variates (package Nimble)
#> this can take tens of minutes with many data or variates.
#> Please wait...
```





## Drawing inferences



*(TO BE CONTINUED)*

 -->
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by PierGianLuca Porta Mana, Aurora Grefsrud, Håkon Mydland, Maksim Ohvrill, Simen Hesthamar Hauge.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
