<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>A crash course in Bayesian nonparametric inference with inferno • inferno</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="A crash course in Bayesian nonparametric inference with inferno">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">inferno</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/vignette_crashcourse.html">A crash course in Bayesian nonparametric inference with inferno</a></li>
    <li><a class="dropdown-item" href="../articles/vignette_mutualinfo.html">Associations of variates with mutual information</a></li>
    <li><a class="dropdown-item" href="../articles/vignette_parkinson.html">Example of use: inferences for Parkinson's Disease</a></li>
    <li><a class="dropdown-item" href="../articles/vignette_start.html">Example of use: inferno for penguins</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/pglpm/inferno/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>A crash course in Bayesian nonparametric inference with inferno</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/pglpm/inferno/blob/main/vignettes/vignette_crashcourse.Rmd" class="external-link"><code>vignettes/vignette_crashcourse.Rmd</code></a></small>
      <div class="d-none name"><code>vignette_crashcourse.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="before-we-start">Before we start<a class="anchor" aria-label="anchor" href="#before-we-start"></a>
</h2>
<p>This vignette gives an introduction and guide to the kinds of
<em>Bayesian nonparametric inference</em> that can be done with
<strong><em>inferno</em></strong>, by means of a concrete example. It
also has the purpose to clarify, by means of examples, the terminology
used in the <strong><em>inferno</em></strong> package.</p>
<p>It is of course impossible to summarize this branch of probability
theory and of statistics in a couple of sections; you’re invited to
learn more for instance from the <a href="https://pglpm.github.io/ADA511" class="external-link">ADA 511</a> course, or from texts
such as Bernardo &amp; Smith’s <a href="https://doi.org/10.1002/9780470316870" class="external-link"><em>Bayesian Theory</em>
(2000)</a> (especially §§ 4.3, 4.4, 4.6), or Jaynes’s <a href="https://doi.org/10.1017/CBO9780511790423" class="external-link"><em>Probability
Theory</em> (2003)</a> (esp. ch. 9); take also a look at Lindley &amp;
Novick <a href="https://doi.org/10.1214/aos/1176345331" class="external-link"><em>The role of
exchangeability in inference</em> (1981)</a>.</p>
<p>Two extremely important warnings before you follow the example:</p>
<ul>
<li>
<p>Some terms may sound familiar to you, but keep in mind that they
may have quite different meanings from what you’re used to. This is
especially true of the terms in <em>italics</em>. When you see a term in
<strong>boldface</strong>, try to understand its meaning from the way
it’s used, rather than assuming the meaning familiar to you.
Unfortunately, different methods and different disciplines use the same
terms in different ways.</p>
<p>In particular keep in mind that the term <strong>probability</strong>
does <em>not</em> mean <strong>frequency</strong>. ‘Probability’ means
‘plausibility’ or ‘degree of belief’; it is a quantification of a
researcher’s uncertainty about some possible fact. There is a connection
between probability and frequency, but they are not the same. The
distinction is important, because one of the main problems in
statistical inference is that <em>we are uncertain about the frequency
of something</em> – the occurrence of a disease, symptom, or other
characteristics – and our goal is to quantify and reduce that
uncertainty as much as possible.</p>
</li>
<li><p>The example that follows is, like every example, very specific.
But try to see the more general, abstract picture behind it, and to see
how its general methodology could be applied to research that is more
interesting to you. From time to time we shall draw analogies with other
examples that may look very different and yet use essentially the same
methodology.</p></li>
</ul>
</div>
<div class="section level2">
<h2 id="penguins">Penguins<a class="anchor" aria-label="anchor" href="#penguins"></a>
</h2>
<p>We are researchers interested in penguins; specifically the
<strong>population</strong> living in the Antarctic islands
<em>Biscoe</em>, <em>Dream</em>, <em>Torgersen</em>. Our research
questions are not yet precisely defined, but some initial questions of
interest are the following. The penguins can be of three species: <a href="https://www.bas.ac.uk/about/antarctica/wildlife/penguins/adelie-penguin/" class="external-link"><em>Adélie</em></a>,
<a href="https://www.bas.ac.uk/about/antarctica/wildlife/penguins/chinstrap-penguin/" class="external-link"><em>Chinstrap</em></a>,
<a href="https://www.bas.ac.uk/about/antarctica/wildlife/penguins/gentoo-penguin/" class="external-link"><em>Gentoo</em></a>;
we’d like to know:</p>
<dl>
<dt>Q1</dt>
<dd>
Do the three species differ, <em>statistically</em>, in some physical or
geographical characteristic such as sex or island of origin? We say
“statistically” because, for instance, there surely are females and
males of every species, but it’s possible that for one of the species
the ratio of females to males is higher than for another species.
</dd>
<dt>Q2</dt>
<dd>
Is there a physical or geographical characteristic that allows us to
make a good guess about a penguin’s species? We say “guess” because, for
instance, from knowing which island a penguin comes from we can’t be
100% sure about that penguin’s species.
</dd>
</dl>
<p>In order to approach these and other future questions, we decide
which set of penguin characteristics we should consider and observe.
These characteristics are called the <strong>variates</strong> of the
population. This set can be extended or reduced later on. Guided by
previous studies, or by some hypotheses we are entertaining, we choose
the following variates, each denoted by a short
<code>codeword</code>:</p>
<ul>
<li>Species (<code>species</code>)</li>
<li>Island (<code>island</code>)</li>
<li>Bill (<code>bill_len</code>): <a href="https://allisonhorst.github.io/palmerpenguins/reference/figures/culmen_depth.png" class="external-link">the
length of a penguin’s “beak”</a> in millimetres</li>
<li>Bill depth (<code>bill_dep</code>): <a href="https://allisonhorst.github.io/palmerpenguins/reference/figures/culmen_depth.png" class="external-link">the
depth of a penguin’s “beak”</a> in millimetres</li>
<li>Flipper length (<code>flipper_len</code>): the length of a penguin’s
“wing” in millimetres</li>
<li>Body mass (<code>body_mass</code>) in grammes</li>
<li>Sex (<code>sex</code>)</li>
</ul>
<p>Later on we shall include an additional variate:</p>
<ul>
<li>Study year (<code>year</code>): the year a penguin was observed and
measured</li>
</ul>
<p>because there could be time trends in the values we observe. For
instance, a particular species might have more females than males during
one year, and vice versa during another year.</p>
<div class="section level3">
<h3 id="population">“Population”?<a class="anchor" aria-label="anchor" href="#population"></a>
</h3>
<p>The question of time-trends leads us to more general considerations
and questions about statistical research, which unfortunately are often
forgotten.</p>
<p>To start with: this “population” which we want to study, how is it
defined exactly? What counts as a member of the population, and what
doesn’t? For example, a seal clearly doesn’t count, because it isn’t a
penguin. A penguin from the Galápagos island doesn’t count either,
because it isn’t from one of the three Antarctic islands we specified.
But does a penguin who was alive in the year 1830 count? what about one
who will live in those islands in the year 2100? Does a penguin with
some kind of notable physical impairment count? Does a penguin who only
lived 1 year count? Does a penguin born in the Galápagos island but
subsequently transported to the Antarctic islands count?</p>
<p>No matter which research field you work in, you realize that
analogous questions appear when you try to define the “population” in
some study.</p>
<p>It is practically impossible to specify an exact criterion for
membership in a population under study. We may try to cover and delimit
as many factors as possible, but there may always appear a new one we
didn’t think of. There’s no “objective” specification of a population:
the specification depends on our research purpose – which often is not
precisely specified either. We must therefore always be prepared to
further specify the population of our study. In some cases we may even
need to modify our previous specification, and thus discard some data or
acquire new ones.</p>
<p>The intrinsic and unavoidable problem in the specification of a
population has important consequences for the way we observe and measure
the population and its variates. We now turn to this problem.</p>
</div>
<div class="section level3">
<h3 id="sampling">Sampling<a class="anchor" aria-label="anchor" href="#sampling"></a>
</h3>
<p>Our questions <strong>Q1</strong> and <strong>Q2</strong> could have
<em>exact statistical answers</em> if we had a <em>complete census</em>
of the penguin population; that is, if we went and “measured” the values
of the variates in each and every penguin of the population.</p>
<p>To make this point clearer, imagine a slightly different population.
Suppose we were interested only in all the penguins alive today on a
specific, very small island. The island turns out to have 17 penguins.
We check all of them. We find that 6 of these are females, and 11 are
males. It’s then a fact that 6/17${}$35.3% of penguins in this specific
population are females, and 64.7% are males. And if someone picked a
penguin from this population and asked us to guess its sex, we would
give a 35.3% probability to that penguin’s being female, and 64.7% to
its being male.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;Note the difference between probability and frequency.
If an expert colleague tells us “I was able to take a quick look at that
penguin, and it looks female to me, though I’m not completely sure”,
then our &lt;em&gt;probability&lt;/em&gt; that this specific penguin is female would
get higher than 50%. If we observe the penguin and see that it’s female,
our probability for female would become 100%. Yet the &lt;em&gt;frequency&lt;/em&gt;
of females in the population is still 35.3%.&lt;/p&gt;"><sup>1</sup></a></p>
<p>But it is often impractical or impossible to take a complete census.
Think of the case were the whole population includes members or variates
that will only exist in the future. For this reason we proceed as
follows: we observe a <strong>sample</strong> of the whole population,
and from the study of this sample we try to infer the statistical
properties of the whole population. Our inferences are perforce
uncertain: we can’t be fully sure about the exact statistical properties
of the whole population. The essential point is that our uncertainty is
not just a matter of “I don’t know” versus “I know”:</p>
<ul>
<li>we can <em>quantify</em> our uncertainty;</li>
<li>we can <em>reduce</em> our uncertainty, sometimes to the point where
it becomes almost a certainty.</li>
</ul>
<p>Some trusted colleagues thus go to the three islands, and examine
samples of penguins there, sending back the values they observe. The way
they do the sampling would require a deep analysis and discussion, but
for the moment we put these aside.</p>
<p>How many samples should our colleagues collect? The short answer is:
as many as they can. The more samples we have, the more certain our
conclusions will be – all kinds of conclusions: for instance those that
may reveal the presence, as well as those that may reveal the absence,
of important associations.</p>
<p>One important feature of Bayesian inference is that <em>we don’t need
to choose beforehand when to stop sampling</em>. We can monitor the
results as more and more samples arrive, and stop as soon as the level
of certainty about the results is satisfactory. This is possible because
in Bayesian inference the <em>sample size affects only the
uncertainty</em> we have of the ground truth – association, effect, or
other hypothesis – but <em>it cannot affect the ground truth
itself</em>. If the truth is that “there is no effect”, then this truth
will come to light with more and more certainty as we increase the
sample size, and no amount of sampling will be able to change this
truth.</p>
</div>
</div>
<div class="section level2">
<h2 id="first-analysis">First analysis<a class="anchor" aria-label="anchor" href="#first-analysis"></a>
</h2>
<p>We receive the first 10 datapoints. They are stored in the file
<code>'penguindata2007_10.csv'</code>, respecting the <a href="#format">formatting rules required by
<strong><em>inferno</em></strong></a>. Here they are:</p>
<table style="width:100%;" class="table">
<colgroup>
<col width="4%">
<col width="14%">
<col width="14%">
<col width="12%">
<col width="12%">
<col width="17%">
<col width="14%">
<col width="10%">
</colgroup>
<thead><tr class="header">
<th align="left"></th>
<th align="left">species</th>
<th align="left">island</th>
<th align="right">bill_len</th>
<th align="right">bill_dep</th>
<th align="right">flipper_len</th>
<th align="right">body_mass</th>
<th align="left">sex</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">Gentoo</td>
<td align="left">Biscoe</td>
<td align="right">47.8</td>
<td align="right">15.0</td>
<td align="right">215</td>
<td align="right">5650</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">Gentoo</td>
<td align="left">Biscoe</td>
<td align="right">45.4</td>
<td align="right">14.6</td>
<td align="right">211</td>
<td align="right">4800</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">Chinstrap</td>
<td align="left">Dream</td>
<td align="right">52.0</td>
<td align="right">18.1</td>
<td align="right">201</td>
<td align="right">4050</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">Chinstrap</td>
<td align="left">Dream</td>
<td align="right">51.3</td>
<td align="right">18.2</td>
<td align="right">197</td>
<td align="right">3750</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">Gentoo</td>
<td align="left">Biscoe</td>
<td align="right">42.9</td>
<td align="right">13.1</td>
<td align="right">215</td>
<td align="right">5000</td>
<td align="left">female</td>
</tr>
<tr class="even">
<td align="left">6</td>
<td align="left">Chinstrap</td>
<td align="left">Dream</td>
<td align="right">46.6</td>
<td align="right">17.8</td>
<td align="right">193</td>
<td align="right">3800</td>
<td align="left">female</td>
</tr>
<tr class="odd">
<td align="left">7</td>
<td align="left">Adelie</td>
<td align="left">Torgersen</td>
<td align="right">39.3</td>
<td align="right">20.6</td>
<td align="right">190</td>
<td align="right">3650</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="left">8</td>
<td align="left">Adelie</td>
<td align="left">Torgersen</td>
<td align="right">38.6</td>
<td align="right">21.2</td>
<td align="right">191</td>
<td align="right">3800</td>
<td align="left">male</td>
</tr>
<tr class="odd">
<td align="left">9</td>
<td align="left">Chinstrap</td>
<td align="left">Dream</td>
<td align="right">51.7</td>
<td align="right">20.3</td>
<td align="right">194</td>
<td align="right">3775</td>
<td align="left">male</td>
</tr>
<tr class="even">
<td align="left">10</td>
<td align="left">Gentoo</td>
<td align="left">Biscoe</td>
<td align="right">48.7</td>
<td align="right">15.1</td>
<td align="right">222</td>
<td align="right">5350</td>
<td align="left">male</td>
</tr>
</tbody>
</table>
<p>With these datapoints we start our Bayesian nonparametric analysis
using <strong><em>inferno</em></strong>! Let’s load the package:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/pglpm/inferno/" class="external-link">inferno</a></span><span class="op">)</span></span></code></pre></div>
<p>There are two preliminary steps that we usually must follow to
perform an analysis:</p>
<ol style="list-style-type: decimal">
<li>Determine and prepare the <strong>metadata</strong> about the
variates of interest.</li>
<li>Perform the general inference about the whole population from the
sampled data; we shall call this <strong>learning</strong> from the
sample data.</li>
</ol>
<div class="section level3">
<h3 id="metadata-preparation">Metadata preparation<a class="anchor" aria-label="anchor" href="#metadata-preparation"></a>
</h3>
<p>Metadata are “data about the data”. They are provided in a CSV file
respecting the <a href="#format">formatting rules</a> and consist of
around eight pieces of information about the variates of our population.
Specifically:</p>
<dl>
<dt>Variate name (<code>name</code>)</dt>
<dd>
This is a character or string: name of a variate. Different variates
should obviously have different names.
</dd>
<dt>Variate type (<code>type</code>)</dt>
<dd>
<p><strong><em>inferno</em></strong> can handle three kinds of
variates:</p>
<ul>
<li>
<code>nominal</code>: it can take on a finite number of discrete
values, which do not have any natural ordering. Examples could be sex or
geographical location.</li>
<li>
<code>ordinal</code>: it can take on a finite number of discrete
values, which do have a natural ordering. They can be qualitative or
numeric. Examples could be the degree of satisfaction of a customer, or
the severity of a disease, or a <a href="https://www.britannica.com/topic/Likert-Scale" class="external-link">Likert
scale</a>.</li>
<li>
<code>continuous</code>: it can in principle take on an infinite
number of continuous values, although they can be discretized or
rounded. Examples could be age or weight.</li>
</ul>
<p>Many other types of variates exist. For example images and audio are
also types of variates; but <strong><em>inferno</em></strong> cannot
handle these complex types. A simple type of variate that
<strong><em>inferno</em></strong> cannot properly handle is the
<em>cyclic</em> one, such as time of day. There are no clear-cut
separations between different types of variates; thus it’s sometimes
difficult to assess the type.</p>
</dd>
<dt>Domain minimum and maximum (<code>domainmin</code>,
<code>domainmax</code>)</dt>
<dd>
<p>These are numbers, only defined for numeric-ordinal and continuous
variates. These metadata are the minimum and maximum value a variate can
take on in a given study. For example, in a study about health or
employment of adults, a variate <em>age</em> might have a minimum of
18 years. In a more general study involving people of all ages, the
minimum would be 0. In a health study where people of 90 years or more
are pooled together, the maximum would be 90 years. A minimum could also
be minus-infinity, and a maximum plus-infinity. Obviously the minimum
should be lower than the maximum.</p>
<p>Sometimes the minimum or maximum is not clear-cut. For instance,
there is no theoretical maximum on a person’s age, although we can
consider an age of 300 impossible to reach today. In such cases the
maximum or minimum can be taken to be plus or minus infinity.</p>
</dd>
<dt>Interval between values (<code>datastep</code>)</dt>
<dd>
This is a positive number, only defined for numeric-ordinal and
continuous variates. It is the separation between consecutive values.
For example, in a 1–5 Likert-scale variate, the interval is 1. For an
<em>age</em> variate expressed in and rounded to years, the interval
is 1. For a <em>length</em> variate expressed in centimetres and rounded
to millimetres, the interval is 0.1.
</dd>
<dt>Inclusion of inimum and maximum (<code>minincluded</code>,
<code>maxincluded</code>)</dt>
<dd>
These are logical or <code>yes</code>/<code>no</code>, only defined for
continuous variates. They tell whether the minimum and maximum of a
continuous variate are themselves possible values or not. Often such
extreme values have a special meaning, and this information is important
with censored or pooled data. For example, a <em>length</em> continuous
variate might be expressed in decimetres and reported without rounding,
but all lengths above 1 dm might be pooled together into the value “1 dm
or more”; in such case we must make sure to state that
<code>maxincluded</code> is <code>True</code> or <code>"yes"</code>.
</dd>
<dt>Values (<code>V1</code>, <code>V2</code>, …)</dt>
<dd>
These are characters or strings, only defined for nominal or non-numeric
ordinal variates. They are the values that a nominal or ordinal variate
can <em>in principle</em> take on in the population of interest. Note
that they are include not only the values that are observed in a sample,
but also those that could be observed in the rest of the population. A
nominal or ordinal variate must have at least two distinct values; it
wouldn’t make much sense to draw inferences about it otherwise.
</dd>
</dl>
<p>Why does <strong><em>inferno</em></strong> need metadata? Because it
performs <em>nonparametric</em> inference. In other words, it does not
assume the frequency distribution of the variates in the whole
population to have any specific class of shapes, such as a Gaussians. It
tries to extrapolate the frequency distribution of the whole population
from the sample data provided. We must therefore provide to it the same
general information that we have about the variates, and about any
artificial modifications performed on their values, such as rounding or
pooling.</p>
<p>In order to prepare the metadata file we can use the
<code><a href="../reference/metadatatemplate.html">metadatatemplate()</a></code> helper function. This function reads the
sample data and prepares a preliminary file containing heuristic
<em>guesses</em> about the metadata, which we’ll have to check and
correct. The function motivates its guesses and warns about especially
uncertain ones:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">datafile</span> <span class="op">&lt;-</span> <span class="st">'penguindata2007_10.csv'</span></span>
<span><span class="va">metadatafile</span> <span class="op">&lt;-</span> <span class="st">'penguinmetadata.csv'</span></span>
<span><span class="fu"><a href="../reference/metadatatemplate.html">metadatatemplate</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">datafile</span>, file <span class="op">=</span> <span class="va">metadatafile</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Analyzing 8 variates for 10 datapoints.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; * "species" variate:</span></span>
<span><span class="co">#&gt;   -  3 different  non-numeric  values detected:</span></span>
<span><span class="co">#&gt;  "Adelie", "Chinstrap", "Gentoo" </span></span>
<span><span class="co">#&gt;   which do not seem to refer to an ordered scale.</span></span>
<span><span class="co">#&gt;   Assuming variate to be NOMINAL.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; * "island" variate:</span></span>
<span><span class="co">#&gt;   -  3 different  non-numeric  values detected:</span></span>
<span><span class="co">#&gt;  "Biscoe", "Dream", "Torgersen" </span></span>
<span><span class="co">#&gt;   which do not seem to refer to an ordered scale.</span></span>
<span><span class="co">#&gt;   Assuming variate to be NOMINAL.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; * "bill_len" variate:</span></span>
<span><span class="co">#&gt;   - Numeric values between 38.6 and 52 </span></span>
<span><span class="co">#&gt;   Assuming variate to be CONTINUOUS.</span></span>
<span><span class="co">#&gt;   - Distance between datapoints is a multiple of 0.1 </span></span>
<span><span class="co">#&gt;   Assuming variate to be ROUNDED.</span></span>
<span><span class="co">#&gt;   - All values are positive</span></span>
<span><span class="co">#&gt;   Assuming "domainmin" to be 0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; * "bill_dep" variate:</span></span>
<span><span class="co">#&gt;   - Numeric values between 13.1 and 21.2 </span></span>
<span><span class="co">#&gt;   Assuming variate to be CONTINUOUS.</span></span>
<span><span class="co">#&gt;   - Distance between datapoints is a multiple of 0.1 </span></span>
<span><span class="co">#&gt;   Assuming variate to be ROUNDED.</span></span>
<span><span class="co">#&gt;   - All values are positive</span></span>
<span><span class="co">#&gt;   Assuming "domainmin" to be 0</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; * "flipper_len" variate:</span></span>
<span><span class="co">#&gt;   - Only 9 different numeric values detected:</span></span>
<span><span class="co">#&gt; from 190 to 222 in steps of 1 </span></span>
<span><span class="co">#&gt;   Assuming variate to be ORDINAL.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; * "body_mass" variate:</span></span>
<span><span class="co">#&gt;   - Only 9 different numeric values detected:</span></span>
<span><span class="co">#&gt; from 3650 to 5650 in steps of 25 </span></span>
<span><span class="co">#&gt;   Assuming variate to be ORDINAL.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; * "sex" variate:</span></span>
<span><span class="co">#&gt;   -  2 different  values detected:</span></span>
<span><span class="co">#&gt;  "female", "male" </span></span>
<span><span class="co">#&gt;   which do not seem to refer to an ordered scale.</span></span>
<span><span class="co">#&gt;   Assuming variate to be NOMINAL.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; * "year" variate:</span></span>
<span><span class="co">#&gt;   Only one value present.</span></span>
<span><span class="co">#&gt; =========</span></span>
<span><span class="co">#&gt; WARNINGS - please make sure to check these variates in the metadata file:</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; * "year" variate does not have at least two distinct values.</span></span>
<span><span class="co">#&gt; Discarded because non-informative.</span></span>
<span><span class="co">#&gt; Please insert its characteristics by hand in the metadata file.</span></span>
<span><span class="co">#&gt; =========</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Saved proposal metadata file as "penguinmetadata.csv"</span></span></code></pre></div>
<p>Here is the preliminary metadata file created by this helper
function:</p>
<table style="width:100%;" class="table">
<colgroup>
<col width="9%">
<col width="8%">
<col width="7%">
<col width="7%">
<col width="6%">
<col width="9%">
<col width="9%">
<col width="5%">
<col width="7%">
<col width="7%">
<col width="2%">
<col width="2%">
<col width="2%">
<col width="2%">
<col width="2%">
<col width="2%">
<col width="3%">
<col width="3%">
</colgroup>
<thead><tr class="header">
<th align="left">name</th>
<th align="left">type</th>
<th align="right">domainmin</th>
<th align="right">domainmax</th>
<th align="right">datastep</th>
<th align="right">minincluded</th>
<th align="right">maxincluded</th>
<th align="left">V1</th>
<th align="left">V2</th>
<th align="left">V3</th>
<th align="right">V4</th>
<th align="right">V5</th>
<th align="right">V6</th>
<th align="right">V7</th>
<th align="right">V8</th>
<th align="right">V9</th>
<th align="right">V10</th>
<th align="right">V11</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">species</td>
<td align="left">nominal</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="left">Adelie</td>
<td align="left">Chinstrap</td>
<td align="left">Gentoo</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">island</td>
<td align="left">nominal</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="left">Biscoe</td>
<td align="left">Dream</td>
<td align="left">Torgersen</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">bill_len</td>
<td align="left">continuous</td>
<td align="right">0</td>
<td align="right"></td>
<td align="right">0.1</td>
<td align="right"></td>
<td align="right"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">bill_dep</td>
<td align="left">continuous</td>
<td align="right">0</td>
<td align="right"></td>
<td align="right">0.1</td>
<td align="right"></td>
<td align="right"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">flipper_len</td>
<td align="left">ordinal</td>
<td align="right">190</td>
<td align="right">222</td>
<td align="right">1.0</td>
<td align="right"></td>
<td align="right"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">body_mass</td>
<td align="left">ordinal</td>
<td align="right">3650</td>
<td align="right">5650</td>
<td align="right">25.0</td>
<td align="right"></td>
<td align="right"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">sex</td>
<td align="left">nominal</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="left">female</td>
<td align="left">male</td>
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>We see that <code><a href="../reference/metadatatemplate.html">metadatatemplate()</a></code> guessed correctly about
the <code>species</code>, <code>island</code>, and <code>sex</code>
variates, assigning them a <code>nominal</code> type and listing all
possible values that can occur in the whole population. Note that if we
knew of an additional species present in the whole population but absent
from the sample, we ought to add it under the <strong>V3</strong>
column.</p>
<p>The guesses about the <code>bill_len</code> and <code>bill_dep</code>
variates are also correct: they are continuous, rounded to the nearest
tenth of millimetre, and cannot be less than zero. The absence of
<strong>domainmax</strong> values means that the theoretical maximum is
plus infinity. We could replace this maximum with something more
realistic, for instance <code>1000</code> mm or less, but typically
these changes do not lead to relevant differences in our inferences. The
absence of <strong>minincluded</strong> means that the minimum,
<code>0</code>, is not a possible value; similarly for
<strong>maxincluded</strong>.</p>
<p>The guess about the <code>flipper_len</code> variate is not correct.
This variate is similar to <code>bill_len</code> but rounded to the
nearest integer. Similarly for the wrongly guessed
<code>body_mass</code> variate, which is rounded in steps of
<code>25</code>.</p>
<p>We must open and edit the metadata file
<code>'penguindata2007_10.csv'</code> with our favourite editor and
correct and complete the guesses of the helper function. In this case we
end up with the following corrected metadata file:</p>
<table style="width:100%;" class="table">
<colgroup>
<col width="9%">
<col width="8%">
<col width="7%">
<col width="7%">
<col width="6%">
<col width="9%">
<col width="9%">
<col width="5%">
<col width="7%">
<col width="7%">
<col width="2%">
<col width="2%">
<col width="2%">
<col width="2%">
<col width="2%">
<col width="2%">
<col width="3%">
<col width="3%">
</colgroup>
<thead><tr class="header">
<th align="left">name</th>
<th align="left">type</th>
<th align="right">domainmin</th>
<th align="right">domainmax</th>
<th align="right">datastep</th>
<th align="right">minincluded</th>
<th align="right">maxincluded</th>
<th align="left">V1</th>
<th align="left">V2</th>
<th align="left">V3</th>
<th align="right">V4</th>
<th align="right">V5</th>
<th align="right">V6</th>
<th align="right">V7</th>
<th align="right">V8</th>
<th align="right">V9</th>
<th align="right">V10</th>
<th align="right">V11</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">species</td>
<td align="left">nominal</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="left">Adelie</td>
<td align="left">Chinstrap</td>
<td align="left">Gentoo</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">island</td>
<td align="left">nominal</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="left">Biscoe</td>
<td align="left">Dream</td>
<td align="left">Torgersen</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">bill_len</td>
<td align="left">continuous</td>
<td align="right">0</td>
<td align="right"></td>
<td align="right">0.1</td>
<td align="right"></td>
<td align="right"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">bill_dep</td>
<td align="left">continuous</td>
<td align="right">0</td>
<td align="right"></td>
<td align="right">0.1</td>
<td align="right"></td>
<td align="right"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">flipper_len</td>
<td align="left">continuous</td>
<td align="right">0</td>
<td align="right"></td>
<td align="right">1.0</td>
<td align="right"></td>
<td align="right"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">body_mass</td>
<td align="left">continuous</td>
<td align="right">0</td>
<td align="right"></td>
<td align="right">25.0</td>
<td align="right"></td>
<td align="right"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">sex</td>
<td align="left">nominal</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="left">female</td>
<td align="left">male</td>
<td align="left"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>Later on we shall add the variate <code>year</code> to the metadata
file.</p>
</div>
<div class="section level3">
<h3 id="learning-and-extrapolating-from-the-sample-data">“Learning” and extrapolating from the sample data<a class="anchor" aria-label="anchor" href="#learning-and-extrapolating-from-the-sample-data"></a>
</h3>
<p>Now comes the essential part of our analysis: from the sample data
and the metadata information,<strong><em>inferno</em></strong> will draw
inferences about the <em>whole</em> population of penguins, including
new penguins (from the same population) that we shall observe in the
future. Subsequent analyses are based on this main inference. The
general theory behind this kind of inference, based on the property of
<em>exchangeability</em> and related mathematical theorems, is explained
in the <a href="#references">references</a>.</p>
<p>This inference is performed by calling the <code><a href="../reference/learn.html">learn()</a></code>
function. Its minimal arguments are three: the sample <code>data</code>,
the <code>metadata</code>, and the name <code>outputdir</code> of a
directory where the inference results should be saved; some additional
information will be suffixed to this directory name. We can also give a
numeric <code>seed</code> argument if we want to reproduce the same
output in another run.</p>
<p>This function does Monte Carlo sampling and is computationally
expensive. It may take minutes, hours, or days to run, depending on the
amount of sample data and the number of variates. In order to speed up
its computations, it tries to use any multi-cores capabilities of the
machine it runs in. The output shown below was obtained from running it
on 15 cores:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">learnt</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/learn.html">learn</a></span><span class="op">(</span></span>
<span>    data <span class="op">=</span> <span class="va">datafile</span>,</span>
<span>    metadata <span class="op">=</span> <span class="va">metadatafile</span>,</span>
<span>    outputdir <span class="op">=</span> <span class="st">'penguin_inference'</span>,</span>
<span>    seed <span class="op">=</span> <span class="fl">16</span>,</span>
<span>    parallel <span class="op">=</span> <span class="fl">15</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Registered doParallelSNOW with 15 workers</span></span>
<span><span class="co">#&gt; Warning: data have additional variates. Dropping them.</span></span>
<span><span class="co">#&gt; Calculating auxiliary metadata</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Learning:  10 datapoints,  7 variates</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;  ********************************************************* </span></span>
<span><span class="co">#&gt;  Saving output in directory</span></span>
<span><span class="co">#&gt;  penguin_inference-251231T130101-vrt7_dat10_smp3600 </span></span>
<span><span class="co">#&gt;  ********************************************************* </span></span>
<span><span class="co">#&gt; Starting Monte Carlo sampling of 3600 samples by 60 chains</span></span>
<span><span class="co">#&gt; in a space of 899 (effectively 3717) dimensions.</span></span>
<span><span class="co">#&gt; Using 15 cores: 60 samples per chain, max 4 chains per core.</span></span>
<span><span class="co">#&gt; Core logs are being saved in individual files.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; C-compiling samplers appropriate to the variates (package Nimble)</span></span>
<span><span class="co">#&gt; this can take tens of minutes. Please wait...</span></span>
<span><span class="co">#&gt; Compiled core 1. Number of samplers: 1012.</span></span>
<span><span class="co">#&gt; Estimating remaining time, please be patient...</span></span>
<span><span class="co">#&gt; Sampling. Core 12 estimated end time: 2025-31-12 13:04</span></span>
<span><span class="co">#&gt; Finished Monte Carlo sampling.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Max number of Monte Carlo iterations across chains: 5033</span></span>
<span><span class="co">#&gt; Max number of used mixture components: 8</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Checking test data</span></span>
<span><span class="co">#&gt; ( #1 #3 #4 #6 #7 #8 #9 #10 )</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; rel. MC standard error: min: 0.019  max: 0.034  mean: 0.026</span></span>
<span><span class="co">#&gt; eff. sample size: min: 860  max: 2700  mean: 1700</span></span>
<span><span class="co">#&gt; needed thinning: min: 2  max: 7  mean: 4.5</span></span>
<span><span class="co">#&gt; average: min: 6.5e-08  max: 3.2e-07  mean: 1.7e-07</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Plotting final Monte Carlo traces and marginal samples...</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Total computation time: 2.2 mins</span></span>
<span><span class="co">#&gt; Average total time per chain: 0.037 mins</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Removing temporary output files.</span></span>
<span><span class="co">#&gt; Finished.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Closing connections to cores.</span></span></code></pre></div>
<p>The initial output gives a summary of the sample size, number of
variates, saving directory, and other computational details. The
“<code>estimated end time</code>” line updates from time to time with a
better estimate of the computation’s end time. Once the computation is
finished, some final information about the underlying Monte Carlo
computation is provided.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;The Monte Carlo expert will notice that the effective
sample size is around 900 or more. The Monte Carlo sampling is designed
to stop when 3600 samples have been obtained with an effective sample
size of at least 400, or equivalently a relative Monte Carlo standard
error under 0.05. This is enough, considering that the statistical
uncertainty in the results can be tens or hundreds of times larger than
this. The desired number of samples and maximal Monte Carlo standard
error can be specified with the &lt;code&gt;nsamples&lt;/code&gt; and
&lt;code&gt;relerror&lt;/code&gt; arguments.&lt;/p&gt;"><sup>2</sup></a> Note that the inference above, with
10 sample data and 7 variates, took a couple of minutes to complete on
15 cores.</p>
<p>The results of this main inference are now stored in the compressed
file ‘<code>learn.rds</code>’ (around 30 MB) within the specified output
directory. The name of this directory is also saved as the final value
of the <code><a href="../reference/learn.html">learn()</a></code> function, which we saved in the
<code>learnt</code> variable in the code above.</p>
<p><br></p>
<p><em>(TO BE CONTINUED)</em></p>
<p><br></p>
</div>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<p>Bernardo, Smith: <a href="https://doi.org/10.1002/9780470316870" class="external-link"><em>Bayesian
Theory</em></a> (repr. 2000). See especially §§ 4.2–4.3, 4.6.</p>
<p>Dawid: <a href="https://doi.org/10.1093/acprof:oso/9780199695607.003.0002" class="external-link"><em>Exchangeability
and its ramifications</em></a> (2013).</p>
<p>de Finetti: <a href="https://www.numdam.org/item/AIHP_1937__7_1_1_0" class="external-link"><em>La prévision:
ses lois logiques, ses sources subjectives</em></a> (1937).</p>
<p>Heath, Sudderth: <a href="https://doi.org/10.1080/00031305.1976.10479175" class="external-link"><em>De Finetti’s
theorem on exchangeable variables</em></a> (1976).</p>
<p>Hewitt, Savage: <a href="https://doi.org/10.1090/S0002-9947-1955-0076206-8" class="external-link"><em>Symmetric
measures on Cartesian products</em></a> (1955).</p>
<p>Lindley, Novick: <a href="https://doi.org/10.1214/aos/1176345331" class="external-link"><em>The role of
exchangeability in inference</em></a> (1981).</p>
<p><br><strong><em>inferno</em></strong> accompanying manual: <a href="https://github.com/pglpm/inferno/raw/main/development/manual/optimal_predictor_machine.pdf" class="external-link"><em>Foundations
of inference under symmetry</em></a> (draft).</p>
<p><br></p>
</div>
<div class="section level2">
<h2 id="format">Format for data and metadata files and their contents<a class="anchor" aria-label="anchor" href="#format"></a>
</h2>
<p>The functions of the <strong><em>inferno</em></strong> package accept
CSV files formatted as follows:</p>
<ul>
<li>Decimal values should be separated by a <em>dot</em>; no comma
should be used to separate thousands etc. Example:
<code>86342.75</code> .</li>
<li>Character and names should be quoted in single or double quotes.
Example: <code>"female"</code>.</li>
<li>Values should be separated by <em>commas</em>, not by tabs or
semicolons.</li>
<li>Missing values should be simply <em>empty</em>, not denoted by “NA”,
“missing”, “-”, or similar.</li>
</ul>
<p>Names of variates and character variate values should be strings
conforming to <a href="https://cran.r-project.org/doc/FAQ/R-FAQ.html#What-are-valid-names_003f" class="external-link">R’s
rules</a>.</p>
<!-- 

## Population inferences

A very common type of inference is the following. We have a large or potentially infinite group of entities, each having a set of characteristics. Having checked the characteristics of several entities from this group, we want to make guesses about the characteristics of new entities from the same group.

The description just given is very generic or even vague. Indeed it can be applied to a huge variety of situations.

- The 'entities' could be: objects, like electronic components; or people, or animals, or plants, or events of some kind.
- The characteristics could be: the material the object is made of, and the result of some test made on it; or the age, blood-test results, and disease condition of the person; or the species and body mass of the animal; or the length of the petals of the flower.
- The group could be: electronic components coming out of a particular production line; or people of a given nationality and with given symptoms; or animals from a particular island; or flowers of a specific genus or family.

The possibilities are endless.

Also the kinds of guesses that we want to make can be very diverse. We might want to guess all characteristics of a new entity; or, having checked *some* characteristics of a new entity, we want to guess the ones that we haven't or can't check. Important examples of this kind of inference appear in medicine. For example we may have a group defined as follows:

- *Group*: people from a given nationality, suffering from one of two possible diseases.

And we may consider these characteristics:

- *Characteristics*: age; sex; weight; presence or absence of a particular genetic factor; symptoms from a specific set of possible ones; results from clinical tests taken at different times; kind of disease.

We observed as many as possible of these characteristics in a sample of people from this group. Now a new person from the same group appears in front of us. We check this person's age, sex, weight, symptoms. We need to guess which of the two diseases affects this person.

\
The kind of inference summarily described above has one important aspect. Suppose you have collected a sample from your group of interest, and you want to use this sample to make guesses about new entities from the same group. If someone exchanged one entity in your sample with another one *unsystematically* chosen from the group, then you wouldn't protest. After all, you still have the same number of samples from the same group. This aspect is called **exchangeability**: we say that this kind of inference is exchangeable. There are kinds of inference for which this aspect is not true. For example, suppose you're given some stock data from four consecutive days, and you want to make guesses about the next day. If someone replaced any of your four datapoints with a datapoint an unsystematically chosen other day of the year, then you would protest: the time order of the datapoints matter. This is an example of non-exchangeable inference.

The inferences for which *inferno* is designed are exchangeable ones. We shall also call them **population inferences**.

\
Having discussed these simple examples, let's agree on some more standard terminology. Let's call:

- **Population**: what we've called 'group'.
- **Unit**: what we've called 'entity' -- the object, person, animal, etc.
- **Variate**: what we've called 'characteristic'.


## Probability and population frequencies

We have been speaking about "making guesses"; but what does this mean?

When a unit is chosen unsystematically from a population, it's only in rare situations that we may be sure about the variates of this unit before checking them. Suppose for instance that the population of interest is 'all adults from a given country'; and the variates of interest are `sex`, `weight`, `height`. If a person is chosen from this population, we can't be sure beforehand of how tall this person will be. We can exclude values like 4 m or 20 cm, but we'll be uncertain about many other possible values. Even if someone tells us the sex and weight of this person, we'll still be in doubt, but maybe we can consider some values more probable than others.

That's the keyword: **probability**. Although we are unsure about a variate of a unit, we can still find some values more probable than others: we may consider it more probable that the person is 160 cm tall than 180 cm tall; or in a clinical inference we may consider it more probable that the patient has a particular virus than not.

Probability is our *degree of belief* about the value of the unknown variate.

Note that probability is not a physical property of the unit. For instance, the person in question may be exactly 158 cm tall; the probability of being 180 cm tall is not something we can "measure" from the person. Also, if we found out some other variate of the person -- say we knew the weight, and now we know also the sex -- then the probability we assign to 180 cm may increase or decrease; but the person is exactly the same as before. Probability is not a property of the population either. For instance, for one person from a population we may think 150 cm to be the most probable height, but for another person from the same population we may think 180 cm to be the most probable height instead.

Probability expresses the *information* we have about the variate of a unit. This is why another researcher may have a different probability about the same unit: because they may have different information about that unit.

\
There is a situation in which we would all agree about the probability about a variate of a unit: when we know the **frequencies** for that variate in the population. Suppose for instance that the population of interest is that of penguins who lived in particular locations in some particular years (penguins from other locations or other times are not part of this population). The variates are the penguins' `species` and the `island` they lived in.

You're told that, as a matter of fact, 43.7% of penguins from the *whole* population are of species *Adelie*, 20.0% of species *Chinstrap*, and 36.3% of species *Gentoo*. Now a penguin from that population is brought in front of you, but you can't see any of its characteristics. What's your degree of belief that this penguin is of species *Adelie*, or *Chinstrap*, or *Gentoo*? We'd all agree, given the frequency information about this population, to assign the probabilities of 43.7%, 20.0%, and 36.3% to the three possibilities, for this penguin. We write this as follows:
$$
\begin{aligned}
&\mathrm{Pr}(\texttt{species}\!=\!\mathit{Adelie} \mid 
\textsf{population frequency}) = 0.437
\\
&\mathrm{Pr}(\texttt{species}\!=\!\mathit{Chinstrap} \mid 
\textsf{population frequency}) = 0.200
\\
&\mathrm{Pr}(\texttt{species}\!=\!\mathit{Gentoo} \mid 
\textsf{population frequency}) = 0.363
\end{aligned}
$$
On the left side of the '$\mid$' bar we write what we're guessing or are unsure about. On the right side, we write the information that led to our probability assignment; in this case, the information about the full population. When the information behind a probability is understood, the '$\mid$' bar and the right side are usually omitted.

In this case, the highest probability is for the *Adelie* species, but the other two possibilities cannot be excluded. The numerical values of these probabilities are extremely important, because they determine any kind of **decision** we may have to make about our unit. This is especially true in medical decision-making, where probabilities, combined with **utilities**, determine which is the best choice that a clinician can make. Medical and clinical decision-making, and the role of probabilities in them, are discussed for instance in the texts by Sox & al.: [*Medical Decision Making* (2024)](https://doi.org/10.1002/9781119627876), by Hunink & al.: [*Decision Making in Health and Medicine* (2014)](https://doi.org/10.1017/CBO9781139506779), or by Weinstein & Fineberg [*Clinical Decision Analysis* (1980)](https://archive.org/details/clinicaldecision0000unse_e3n8).

The example with the penguin population can be analysed further. The penguins in this population come from three possible `island`s: *Biscoe*, *Dream*, and *Torgersen*. Therefore a penguin can be of one of three species and from one of three islands, for a total of 3 × 3 = 9 possibilities. You're told that, as a matter of fact, the frequencies of these nine combinations in the whole population are as follows:

|             |          |             |          |
|:-----------:|:--------:|:-----------:|:--------:|
|             | *Adelie* | *Chinstrap* | *Gentoo* |
| *Biscoe*    | 12.6%    | 1.8%        | 34.0%    |
| *Dream*     | 17.4%    | 16.5%       | 1.4%     |
| *Torgersen* | 13.8%    | 1.6%        | 0.9%     |

Then we'd all agree that the probability that the penguin brought to us is of `species` *Gentoo* and from `island` *Biscoe* would be 34.0%:
$$
\mathrm{Pr}(
\texttt{species}\!=\!\mathit{Gentoo} ,
\texttt{island}\!=\!\mathit{Biscoe} 
\mid 
\textsf{population frequency}) = 0.340
$$

\
***inferno*** allows you to calculate probabilities of this kind, for any set of variates of your choice.

## Learning from known variates

But there's even more interesting information in the population frequencies above. Let's focus on *Biscoe* island. With a quick sum we see that 48.4% of the whole population comes from *Biscoe* island (thus we'd assign a probability of 0.484 that our penguin comes from that island). Dividing the frequencies above, row-wise, by the frequencies of the respective islands (the sums of each row), we can find the frequency of each species *for each particular island*:

|                  |          |             |          |
|:----------------:|:--------:|:-----------:|:--------:|
|                  | *Adelie* | *Chinstrap* | *Gentoo* |
| from *Biscoe*    | 26.03%   | 3.72%       | 70.25%   |
| from *Dream*     | 49.29%   | 46.74%      | 3.97%    |
| from *Torgersen* | 84.66%   | 9.82%       | 5.52%    |

 We call these **conditional frequencies**, and we call the group of penguins that come from `island` *Biscoe*a **subpopulation** of the whole population. The table above reports the frequencies of the three `species` in each subpopulation.

Thus we also know, for instance, that 70.25% of penguins in the subpopulation from `island` *Biscoe* are of `species` *Gentoo*. This species is the majority in that subpopulation -- contrast this with the majority in the whole population, which we saw was *Adelie*. Indeed, your most probable guess about the `species` of the penguin in front of you was *Adelie*, with 0.437 probability. 

But suppose now someone tells you that this penguin comes from `island` *Biscoe* (so this variate is now known to you). Given this new piece of information, which probabilities do you assign to the `species` of this penguin? Obviously 0.2603 for *Adelie*, 0.0372 for *Chinstrap*, and 0.7025 for *Gentoo*. We write this as follows:
$$
\begin{aligned}
&\mathrm{Pr}(\texttt{species}\!=\!\mathit{Adelie} \mid 
\texttt{island}\!=\!\mathit{Biscoe} ,
\textsf{population frequency}) = 0.2603
\\
&\mathrm{Pr}(\texttt{species}\!=\!\mathit{Chinstrap} \mid 
\texttt{island}\!=\!\mathit{Biscoe} ,
\textsf{population frequency}) = 0.0372
\\
&\mathrm{Pr}(\texttt{species}\!=\!\mathit{Gentoo} \mid 
\texttt{island}\!=\!\mathit{Biscoe} ,
\textsf{population frequency}) = 0.7025
\end{aligned}
$$
The right side of the '$\mid$' bar now reports the extra information that the penguin comes from `island` *Biscoe*. We say that our probability has been *updated*, and we call it a **conditional probability**.

Learning about the penguin's `island` not only made you change the highest probability assignment from *Aelie* to *Gentoo*, but it also increased the value of the highest probability. Without knowing the penguin's `island`, *Adelie* had slightly less than 50% probability; it was as likely as not. After learning the `island` variate, *Gentoo* gets more than 70% probability.

An analogous discussion can be made for a continuous variate. Let's for instance take the `body_mass` of the penguins, discretized in steps of 0.025 kg. Suppose we knew that the histogram of body masses in the whole population were as follows:

<img src="/home/runner/work/inferno/inferno/vignettes/prob-body_mass.jpg" width="75%" style="display: block; margin: auto;" />

Then, if you had to guess the body mass of the penguin brought to you, you'd give the value 4 kg (plus or minus 0.0125 kg) a probability of around 1%, and so on for other possible values; the most probable value being 3.6 kg at 1.4% probability.

Suppose you're now given also the histograms for the subpopulations from the three islands:

<img src="/home/runner/work/inferno/inferno/vignettes/prob-body_mass-given_island.jpg" width="75%" style="display: block; margin: auto;" />

Upon learning that your penguin is from *Biscoe*, your degree of belief would change: the most probable value would be 4.8 kg at 1.3% probability. Note how the histograms for `body_mass` are different in the subpopulations of the three islands. In *Torgersen*, for instance, it's more probable to find penguins between 3 kg and 4 kg than between 5 kg and 6 kg; whereas in *Biscoe* the opposite is true.

\medskip

This is exactly the kind of learning situation that takes place in medicine and in clinical inferences. A clinician searches for symptoms because they may change and increase the probabilities of different diseases or health conditions. The reason why clinicians research particular *subpopulations* -- particular demographics, or genetic factors, etc. -- is that within these subgroups the probability that a medical condition exist or will occur can be drastically higher. In turn, this leads the clinicians to understand better what can be the biological relationships between the health condition and those factors.

\
***inferno*** allows you to calculate the updated probabilities of any set of variates, conditional on any other set.


## Uncertainty about whole populations. Population samples

Inferences are therefore quite clear if we know the frequencies of the variates for the *whole* population. Our main problem is that we usually *don't know those whole-population frequencies*. For example, do you know the *exact* number of people, among those born in your country at any time and are alive today, who are exactly between 165 cm and 170 cm tall today?

For our penguin example, let's consider again the `body_mass` variate. We actually don't know what the histogram of this variate over the whole population looks like. It could have one or several peaks, a symmetric or non-symmetric shape, shoulders, an so on. Here are seven possibilities:

<img src="/home/runner/work/inferno/inferno/vignettes/penguin_priors.jpg" width="75%" style="display: block; margin: auto;" />

\
It's often impractical or impossible to measure the frequencies of a variate in a whole population. We must therefore *guess* them.

In order to guess them, we usually examine a **sample** from the population of interest. This sample is chosen in an unsystematic way, so that its statistics and its **sample frequencies** are not affected by peculiar choices, otherwise we would have a *biased* sample (imagine for instance choosing a sample of *male* penguins only, whereas the penguin population we're interested in has both males and females).

But the frequencies of variates in a sample from a population are typically different from those in the whole population. The difference is the larger, the smaller the sample. For example, if we chose five penguins from our penguin population in an unsystematic way, it could happen that we got five penguins all of species *Gentoo*. Then the variate `species` would have frequencies of 100% for *Gentoo* and 0% for *Adelie* and *Chinstrap*. But of course this would not mean that only the species *Gentoo* occurs in the whole population. In order to reflect the frequencies in the whole population, the size of a sample needs to be larger, the larger are:

- the numbers of variates considered;
- the possible values of each variate.

Therefore, to guess the frequencies in the whole population we can examine a sample from it -- but we cannot fully rely on the sample.

\medskip

The traditional way of facing this uncertainty was to *assume* that the whole population has frequencies with particular features; most commonly, they were assumed to be *Gaussian*. The variate values observed in the sample were used to fit the free parameters of the particular assumed distribution, like the means, variances, covariances of the Gaussian.

Consider how drastic such an approach is. It can also be misleading, if one is not aware of what is being done. For instance, errors can be reported about the fit of the free parameters, and if they are small they can give a false sense of precise inference. But the very *assumption* has errors, which may be quite large; and such errors are rarely reported.

One could object that the assumption, say of gaussianity, behind such methods is not completely arbitrary: one can get the idea of whether it's correct or not by looking at the sample. But this objection is self-contradictory: as discussed, our problem is that the frequencies in the sample are not a faithful reflection of those in the whole population. We're making assumption because the sample is not reliable, so how can we rely on the sample to judge those assumptions?

It must be pointed out that even methods or measures that do not explicitly refer to peculiar assumptions about frequencies, *do* actually rely on such hidden assumptions. Pearson's measure of correlation, for instance, actually relies on the assumption that the joint frequency distribution for the whole population is Gaussian.
 add ref 

## Bayesian nonparametric population inference

The method based on the assumption of peculiar frequency distribution was a necessity in the past, because there was no other computationally feasible way of facing the uncertainty about the whole population. But today the situation is different.

Instead of making peculiar assumptions out of computational desperation, today we can explicitly recognize our uncertainty about the frequencies in the whole population, and therefore account for the errors that can come from this uncertainty. This is what Bayesian nonparametric population inference does. The term 'nonparametric' means that no peculiar assumptions are made about frequency distribution having simple shapes (and therefore expressible with few parameters).

Since we must *guess* what the frequencies are in the whole population, we proceed by assigning *probabilities* to all possible such frequency distributions. Intuitively, a candidate frequency distribution is more probable:

- the more it fits the frequencies measured in the sample;
- the more it looks "natural".

An example of unnatural or strange probability distribution, for our `body_mass` variate, could be the following:

<img src="/home/runner/work/inferno/inferno/vignettes/penguin_prior_strange.jpg" width="75%" style="display: block; margin: auto;" />

It seems quite unnatural based on our experience with histograms over large natural populations. If a sample from a penguin population showed this frequency distribution of `body_mass` values, you would probably check whether any errors were made during the sampling.

The core of the ***inferno*** package is the calculation of the probabilities for the possible frequencies of the whole population, and the selection of a large sample of the most probable ones. This calculation and sampling are realized by the `learn()` function.

## Back to guessing about the next unit






There are situations in which it is not possible to choose the sample in an unsystematic way. Yet such a biased sample can still be used for population inference, as long as we know what its bias may be. ***inferno*** allows you to make this kind of bias corrections as well.

But the frequencies of variates in a sample from a population are typically different from those in the whole population. The difference is the larger, the smaller the sample. There are even extreme situations in which some statistics from a sample, like the mean, can be way off those of the whole population, even if the sample is large (the statistics become reliable only when the sample is almost as large as the full population). The plot below shows an example. The variate is a real number that can take on positive or negative values, in a population of 100 000 000 units; the mean value of the variate in the whole population is $-1.14$. The plot shows the values of the mean obtained from unsystematically chosen samples of various sizes. Even in a sample of 30 000 000 units we may observe a mean of $-3$. The *median* of a sample is much more reliable.

<img src="/home/runner/work/inferno/inferno/vignettes/sample_mean.jpg" width="67%" style="display: block; margin: auto;" />

The point of this example is only to make clear that although a sample from a population gives us some information about the whole population, we are still uncertain about the whole-population frequencies.


<!-- ![](sample_mean.jpg){#id .class width=50%}

*(TO BE CONTINUED)*


----

(OLD TEXT)



We store the name of the datafile

``` r
dataset <- penguins
```
Here are the values for two subjects:


|    |species   |island    | bill_len| bill_dep| flipper_len| body_mass|sex    | year|
|:---|:---------|:---------|--------:|--------:|-----------:|---------:|:------|----:|
|225 |Gentoo    |Biscoe    |     48.2|     15.6|         221|      5100|male   | 2008|
|127 |Adelie    |Torgersen |     38.8|     17.6|         191|      3275|female | 2009|
|315 |Chinstrap |Dream     |     46.9|     16.6|         192|      2700|female | 2008|
|189 |Gentoo    |Biscoe    |     42.6|     13.7|         213|      4950|female | 2008|
|271 |Gentoo    |Biscoe    |     47.2|     13.7|         214|      4925|female | 2009|



### "Natural" vs controlled or "biased" variates

## Metadata

Let's load the package:

``` r
library(inferno)
```


```{.r .fold-hide}
## metadatatemplate(data = datafile, file = 'temp_metadata')
```


``` r
metadatafile <- 'meta_toydata.csv'
```
The metadata are as follows; `NA` indicate empty fields:


|name                                    |type       | datastep| domainmin| domainmax|minincluded |maxincluded |V1     |V2      |
|:---------------------------------------|:----------|--------:|---------:|---------:|:-----------|:-----------|:------|:-------|
|TreatmentGroup                          |nominal    |         |          |          |            |            |NR     |Placebo |
|Sex                                     |nominal    |         |          |          |            |            |Female |Male    |
|Age                                     |continuous |        1|         0|          |            |            |       |        |
|Anamnestic.Loss.of.smell                |nominal    |         |          |          |            |            |No     |Yes     |
|History.of.REM.Sleep.Behaviour.Disorder |nominal    |         |          |          |            |            |No     |Yes     |
|MDS.UPDRS..Subsection.III.V1            |ordinal    |        1|         0|       132|yes         |yes         |       |        |
|diff.MDS.UPRS.III                       |ordinal    |        1|      -132|       132|yes         |yes         |       |        |



*(More to be written here!)*


## Learning

$$
\mathrm{Pr}(Y = y \:\vert\: X = x, \mathrm{data})
$$
As the notation above indicates, *these probabilities also depend on the $\mathrm{data}$ already observed*. They are usually called "posterior probabilities".

We need to prepare the software to perform calculations of posterior probabilities given the observed data. In machine learning an analogous process is called "learning". For this reason the function that achieves this is called `learn()`. It requires at least three arguments:

- `data`, which can be given as a path to the `csv` file containing the data
- `metadata`, which can also be given as a path to the metadata file
- `outputdir`: the name of the directory where the output should be saved.

It may be useful to specify two more arguments:

- `seed`: the seed for the random-number generator, to ensure reproducibility
- `parallel`: the number of CPUs to use for the computation

Alternatively you can set the seed with `set.seed()`, and start a set of parallel workers with the `parallel::makeCluster()` and `doParallel::registerDoParallel()` commands.

The "learning" computation can take tens of minutes, or hours, or even days depending on the number of variates and data in your inference problem. The `learn()` function outputs various messages on how the computation is proceeding. As an example:


``` r
learnt <- learn(
    data = datafile,
    metadata = metadatafile,
    outputdir = 'parkinson_computations',
    seed = 16,
    parallel = 12)

#> Registered doParallelSNOW with 12 workers
#>
#> Using 30 datapoints
#> Calculating auxiliary metadata
#>
#> **************************
#> Saving output in directory
#> parkinson_computations
#> **************************
#> Starting Monte Carlo sampling of 3600 samples by 60 chains
#> in a space of 703 (effectively 6657) dimensions.
#> Using 12 cores: 60 samples per chain, 5 chains per core.
#> Core logs are being saved in individual files.
#>
#> C-compiling samplers appropriate to the variates (package Nimble)
#> this can take tens of minutes with many data or variates.
#> Please wait...
```





## Drawing inferences



*(TO BE CONTINUED)*

 -->
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by PierGianLuca Porta Mana, Aurora Grefsrud, Håkon Mydland, Maksim Ohvrill, Simen Hesthamar Hauge.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
