<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Associations of variates with mutual information • inferno</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Associations of variates with mutual information">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">inferno</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.2.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/vignette_crashcourse.html">A crash course in population inference</a></li>
    <li><a class="dropdown-item" href="../articles/vignette_mutualinfo.html">Associations of variates with mutual information</a></li>
    <li><a class="dropdown-item" href="../articles/vignette_parkinson.html">Example of use: inferences for Parkinson's Disease</a></li>
    <li><a class="dropdown-item" href="../articles/vignette_start.html">Example of use: inferno for penguins</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/pglpm/inferno/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Associations of variates with mutual information</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/pglpm/inferno/blob/main/vignettes/vignette_mutualinfo.Rmd" class="external-link"><code>vignettes/vignette_mutualinfo.Rmd</code></a></small>
      <div class="d-none name"><code>vignette_mutualinfo.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="brief-overview-of-the-theory-behind-mutual-information">Brief overview of the theory behind Mutual Information<a class="anchor" aria-label="anchor" href="#brief-overview-of-the-theory-behind-mutual-information"></a>
</h2>
<div class="section level3">
<h3 id="association-and-traditional-measures-for-it">Association and traditional measures for it<a class="anchor" aria-label="anchor" href="#association-and-traditional-measures-for-it"></a>
</h3>
<p>A probability quantifies our degree of belief in the <em>values</em>
of some variates, given that we know the <em>values</em> of some other
variates. This belief also reflect our uncertainty: maximal for a belief
of 50%, minimal for a belief of 0% or 100%.</p>
<p>In some cases we wish to quantify our uncertainty <em>in an average
sense</em>, rather than about specific values, or given knowledge of
specific values. In other words, we ask: on average, how uncertain are
we about a set of variates, given that we have knowledge of another set?
If this average uncertainty is low, then we say that the two sets of
variates are highly <em>correlated</em> or <em>associated</em>;
otherwise, that the are <em>uncorrelated</em>. Here we are using the
word ‘correlation’ in its general sense, not in the specific sense of
“linear correlation” or similar.</p>
<p>Traditionally the <a href="https://mathworld.wolfram.com/CorrelationCoefficient.html" class="external-link">Pearson
correlation coefficient
“<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>”</a>
is used to quantify association. It is extremely limited, however. It is
essentially <a href="https://doi.org/10.1080/01621459.1954.10501231" class="external-link">based on the
assumption that all variates involved have a joint Gaussian
distribution</a>. As a consequence, it is a measure of <em>linear</em>
association, rather than general association. For instance, if the graph
of two continuous variates
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
is a semicircle, then it means that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
is a function of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>,
and is therefore perfectly associated with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>:
if we know
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>,
then we can perfectly predict the value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.
Yet the Pearson correlation coefficient between the two variates is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math>
in this case, simply because the functional dependence of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
on
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
is not linear:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span>, length.out <span class="op">=</span> <span class="fl">200</span><span class="op">)</span></span>
<span><span class="va">Y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">X</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="va">r</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/cor.html" class="external-link">cor</a></span><span class="op">(</span><span class="va">X</span>, <span class="va">Y</span>, method <span class="op">=</span> <span class="st">'pearson'</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">X</span>, y <span class="op">=</span> <span class="va">Y</span>, type <span class="op">=</span> <span class="st">'p'</span>, main <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="st">'Pearson correlation: '</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html" class="external-link">signif</a></span><span class="op">(</span><span class="va">r</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="vignette_mutualinfo_files/figure-html/unnamed-chunk-1-1.png" width="100%">
Similar limitations of the Pearson correlation coefficient are
demonstrated by the <a href="https://doi.org/10.2307/2682899" class="external-link">“Anscombe
quartet”</a> of datasets.</p>
<p>In the case of two variates, the limitations and possibly misleading
value of the Pearson correlation coefficient may not be a serious
problem, because we can visually inspect the relation and association
between the variates. But in dealing with multi-dimensional variates,
visualization is not possible and we must rely on the numerical values
of the measure we use to quantify “association”. In such cases the
Pearson correlation coefficient can be highly misleading, and let us
conclude that there’s no association when actually there’s a very strong
one. Considering that our world is highly non-linear, this possibility
is far from remote.</p>
</div>
<div class="section level3">
<h3 id="mutual-information">Mutual Information<a class="anchor" aria-label="anchor" href="#mutual-information"></a>
</h3>
<p>But there is a measure of association that does not suffer from the
limitations discussed above: the <strong>mutual information</strong>
between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.
It has in fact the following important properties:</p>
<ul>
<li><p>If there is no association whatever between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>,
in the sense that knowledge of one never changes our probabilities about
the other, then the mutual information between them is zero. Vice versa,
if the mutual information is zero, then there is no association whatever
between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.</p></li>
<li><p>If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
have a perfect association, that is, if knowing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
gives us perfect knowledge about
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
or vice versa, or in yet other words if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
is a function of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
or vice versa, then the mutual information between them takes on its
maximal possible value. Vice versa, if the mutual information takes on
its maximal value, then one variate is a function of the other.</p></li>
</ul>
<p>Regarding the last feature, note that the functional dependence
between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
can be <em>of any kind</em>, not just linear. This means that in the
case of the semicircle plot above, the mutual information between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
has its maximal value.</p>
<p>There’s also a special case in which the <em>maximal</em> value of
the mutual information is zero, which indicates that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
is, technically speaking, a function of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>,
yet knowledge of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
doesn’t give us any knowledge about
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.
It’s the case where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
has just one value, independently of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>:</p>
<p><img src="vignette_mutualinfo_files/figure-html/unnamed-chunk-2-1.png" width="100%"></p>
<p><br></p>
<p>Mutual information is measured in <em>shannons</em> (symbol
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">h</mi></mrow><annotation encoding="application/x-tex">\mathrm{Sh}</annotation></semantics></math>),
or <em>hartleys</em> (symbol
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">H</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi><mi mathvariant="normal">t</mi></mrow><annotation encoding="application/x-tex">\mathrm{Hart}</annotation></semantics></math>),
or <em>natural units</em> (symbol
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">n</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi></mrow><annotation encoding="application/x-tex">\mathrm{nat}</annotation></semantics></math>).
These units and other properties of the mutual information are <a href="https://archive.org/details/iso-standards-collection/BS%20EN%2080000-13-2008%20-%201%20Quantities%20and%20units" class="external-link">standardized
by the International Organization for Standardization (ISO) and the
International Electrotechnical Commission (IEC)</a>. The minimum
possible value of the mutual information is always zero. The maximum
possible value depends instead on the variates
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.</p>
</div>
<div class="section level3">
<h3 id="interpretation-of-the-values-of-mutual-information">Interpretation of the values of mutual information<a class="anchor" aria-label="anchor" href="#interpretation-of-the-values-of-mutual-information"></a>
</h3>
<p>The value of the mutual information between two variates has an
operational meaning and interpretation. To understand it we must first
summarize the meaning of the <em>Shannon information</em> as a measure
of uncertainty or of acquired information.</p>
<p>A value of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>s</mi><annotation encoding="application/x-tex">s</annotation></semantics></math>
shannons represents the uncertainty <em>equivalent</em> to not knowing
which among
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>2</mn><mi>s</mi></msup><annotation encoding="application/x-tex">2^s</annotation></semantics></math>
possible alternatives is the true one. So
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">h</mi></mrow></mrow><annotation encoding="application/x-tex">1\,\mathrm{Sh}</annotation></semantics></math>
represents the uncertainty between two equally possible cases, like the
outcome of a coin toss. Other examples:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">h</mi></mrow></mrow><annotation encoding="application/x-tex">3\,\mathrm{Sh}</annotation></semantics></math>
represent being uncertain among 8 equally possible alternatives;
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1.58</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">h</mi></mrow></mrow><annotation encoding="application/x-tex">1.58\,\mathrm{Sh}</annotation></semantics></math>
represent being uncertain among 3 equally possible alternatives. If a
clinician is uncertain about which disease, among four equally possible
ones, affects a patient, then the clinician’s uncertainty is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">h</mi></mrow></mrow><annotation encoding="application/x-tex">2\,\mathrm{Sh}</annotation></semantics></math>.</p>
<p>The “equivalence” in the definition above reflects the fact that some
possibilities may have higher probabilities than others. For instance,
if there are three alternatives but one of them has 0% probability, then
our uncertainty is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">h</mi></mrow></mrow><annotation encoding="application/x-tex">1\,\mathrm{Sh}</annotation></semantics></math>,
the same as for <em>two</em> equally possible alternatives. If there are
three possibilities with probabilities 11.4%, 11.4%, and 77.2%, then the
uncertainty is approximately
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">h</mi></mrow></mrow><annotation encoding="application/x-tex">1\,\mathrm{Sh}</annotation></semantics></math>:
as if there were only two alternatives; this is because one alternative
has much higher probability than the other two.</p>
<p><br></p>
<p>Back to mutual information: If the mutual information between
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>
amounts to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">h</mi></mrow></mrow><annotation encoding="application/x-tex">h\,\mathrm{Sh}</annotation></semantics></math>,
then knowledge of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math>
reduces, on average,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>2</mn><mi>h</mi></msup><annotation encoding="application/x-tex">2^h</annotation></semantics></math>
times the number of uncertain possibilities of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>.
For example, suppose that a clinician is uncertain about four possible
diseases in a diagnosis (uncertainty of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">h</mi></mrow></mrow><annotation encoding="application/x-tex">2\,\mathrm{Sh}</annotation></semantics></math>),
but there is a particular clinical indicator that associated with the
disease. If the mutual information between the indicator and the disease
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">h</mi></mrow></mrow><annotation encoding="application/x-tex">1\,\mathrm{Sh}</annotation></semantics></math>,
then the clinician will be roughly uncertain about two possible
diseases, rather than four, upon testing the clinical indicator. If the
mutual information were
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">h</mi></mrow></mrow><annotation encoding="application/x-tex">2\,\mathrm{Sh}</annotation></semantics></math>
instead, then the indicator would tell the disease with certainty.</p>
<p><br></p>
<p>You might think: “the value of mutual information is difficult to
interpret”. But this is only a matter of getting familiar with it, by
using it. Remember how difficult it was to get a feeling of what a
particular value of the Pearson correlation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>r</mi><annotation encoding="application/x-tex">r</annotation></semantics></math>
really meant, when you first learned it. You developed an intuitive
understanding of its range of values simply by using it. The same holds
for mutual information.</p>
<p><br></p>
<p>For further discussion about mutual information and entropic measures
of association, take a look at <a href="https://pglpm.github.io/ADA511/information.html" class="external-link"><em>Information,
relevance, independence, association</em></a> and the references given
there.</p>
</div>
</div>
<div class="section level2">
<h2 id="function-for-the-calculation-of-mutual-information-between-variates">Function for the calculation of mutual information between
variates<a class="anchor" aria-label="anchor" href="#function-for-the-calculation-of-mutual-information-between-variates"></a>
</h2>
<p>The software offers the function <code><a href="../reference/mutualinfo.html">mutualinfo()</a></code> to
calculate the mutual information between a set of joint variates
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Y</mi><mn>1</mn></msub><annotation encoding="application/x-tex">Y_1</annotation></semantics></math>
and another set
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Y</mi><mn>2</mn></msub><annotation encoding="application/x-tex">Y_2</annotation></semantics></math>.
It requires at least three arguments:</p>
<ul>
<li>
<code>Y1names</code>: a vector the names (strings) of the variates
in the first set;</li>
<li>
<code>Y2names</code>: a vector the names of the variates in the
second set;</li>
<li>
<code>learnt</code>: the knowledge acquired from the dataset,
calculated with the <code><a href="../reference/learn.html">learn()</a></code> function.</li>
</ul>
<p>It may be useful to specify two more arguments:</p>
<ul>
<li>
<code>parallel</code>: the number of CPUs to use for the
computation;</li>
<li>
<code>nsamples</code>: the number of samples to use for the
computation of the mutual information (default 3600).</li>
</ul>
<p>The output of this function is a list of several entropic measures
and other information. For this example we can focus on two outputs:</p>
<ul>
<li>
<code>$MI</code>: a vector of two numbers: <code>value</code> is the
value of the mutual information in shannons; <code>error</code> is its
numerical error (coming from the Monte Carlo calculation);</li>
<li>
<code>$MImax</code>: a vector of two numbers: <code>value</code> is
the value of the maximal possible value of the mutual information, in
shannons; <code>error</code> is its numerical error.</li>
</ul>
<div class="section level3">
<h3 id="example">Example<a class="anchor" aria-label="anchor" href="#example"></a>
</h3>
<p>As an example, let’s take the dataset and inference problem discussed
in the vignette <a href="vignette_parkinson.html"><em>Example of use:
inferences for Parkinson’s Disease</em></a>.</p>
<p>Suppose we want to quantify the association between the treatment
group of a patient and the effect of the treatment. The treatment group
is given by the <code>TreatmentGroup</code> binary variate, and the
effect is quantified by the difference in MDS-UPDRS-III scores between
the first and second visits, in the variate
<code>diff.MDS.UPRS.III</code>. We use the knowledge calculated and
saved in the <code>learnt.rds</code> file.</p>
<p>The mutual information between <code>TreatmentGroup</code> and
<code>diff.MDS.UPRS.III</code> is</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mutualinfoGroupEffect</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mutualinfo.html">mutualinfo</a></span><span class="op">(</span></span>
<span>    Y1names <span class="op">=</span> <span class="st">'diff.MDS.UPRS.III'</span>,</span>
<span>    Y2names <span class="op">=</span> <span class="st">'TreatmentGroup'</span>,</span>
<span>    learnt <span class="op">=</span> <span class="st">'learnt.rds'</span>,</span>
<span>    parallel <span class="op">=</span> <span class="fl">12</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Registered doParallelSNOW with 12 workers</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Closing connections to cores.</span></span>
<span></span>
<span><span class="va">mutualinfoGroupEffect</span><span class="op">$</span><span class="va">MI</span></span>
<span><span class="co">#&gt;       value       error </span></span>
<span><span class="co">#&gt; 0.002816052 0.001065887</span></span>
<span></span>
<span><span class="va">mutualinfoGroupEffect</span><span class="op">$</span><span class="va">MImax</span></span>
<span><span class="co">#&gt;        value        error </span></span>
<span><span class="co">#&gt; 9.998692e-01 9.717656e-05</span></span></code></pre></div>
<p>The result is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">h</mi></mrow></mrow><annotation encoding="application/x-tex">0\,\mathrm{Sh}</annotation></semantics></math>
within the numerical error, out of a possible maximal value of around
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mspace width="0.167em"></mspace><mrow><mi mathvariant="normal">S</mi><mi mathvariant="normal">h</mi></mrow></mrow><annotation encoding="application/x-tex">1\,\mathrm{Sh}</annotation></semantics></math>,
shows that there is essentially no association between the treatment
group and the effect. Note that further data could change this
conclusion, though (<em>the calculation of this uncertainty will be
later implemented in the package</em>).</p>
<p><br></p>
<p>We can also quantify the association between two sets of variates
<em>within a subpopulation</em> defined by a specific value of an
additional variate <code>X</code> (which may be a joint variate). This
subpopulation is specified by an additional argument <code>X</code> to
the <code><a href="../reference/mutualinfo.html">mutualinfo()</a></code> function. For instance, let’s quantify the
association between treatment group and effect within the subpopulation
of female patients only:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mutualinfoGroupEffectFemales</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mutualinfo.html">mutualinfo</a></span><span class="op">(</span></span>
<span>    Y1names <span class="op">=</span> <span class="st">'diff.MDS.UPRS.III'</span>,</span>
<span>    Y2names <span class="op">=</span> <span class="st">'TreatmentGroup'</span>,</span>
<span>    X <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>Sex <span class="op">=</span> <span class="st">'Female'</span><span class="op">)</span>,</span>
<span>    learnt <span class="op">=</span> <span class="st">'learnt.rds'</span>,</span>
<span>    parallel <span class="op">=</span> <span class="fl">12</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Registered doParallelSNOW with 12 workers</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Closing connections to cores.</span></span>
<span></span>
<span><span class="va">mutualinfoGroupEffectFemales</span><span class="op">$</span><span class="va">MI</span></span>
<span><span class="co">#&gt;       value       error </span></span>
<span><span class="co">#&gt; 0.002208982 0.001186844</span></span>
<span></span>
<span><span class="va">mutualinfoGroupEffectFemales</span><span class="op">$</span><span class="va">MImax</span></span>
<span><span class="co">#&gt;        value        error </span></span>
<span><span class="co">#&gt; 1.0000029105 0.0001987814</span></span></code></pre></div>
<p>The resulting mutual information is above zero, within twice the
numerical error. So there’s a slightly larger – though still extremely
weak – association between the treatment group and effect within the
female subpopulation.</p>
<p><em>(TO BE CONTINUED)</em></p>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by PierGianLuca Porta Mana, Aurora Grefsrud, Håkon Mydland, Maksim Ohvrill, Simen Hesthamar Hauge.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.2.</p>
</div>

    </footer>
</div>





  </body>
</html>
