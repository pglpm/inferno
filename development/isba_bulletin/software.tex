\input{preamble.tex}

\newcommand{\citebi}{\cite}
\newcommand{\citein}{\cite}
\newcommand*{\subtitleproc}[1]{}
\newcommand*{\chapb}{}
\providecommand{\doi}[1]{}
\renewcommand{\doi}[1]{\href{https://doi.org/#1}{doi:#1}}
\newcommand*{\osfdoi}[1]{Open Science Framework \doi{#1}}
\newcommand*{\amp}{\&}
%% Probability macros
\renewcommand*{\P}{\mathrm{P}}%probability
\renewcommand*{\|}[1][]{\nonscript\:#1\vert\nonscript\:\mathopen{}}
\newcommand*{\mo}[1][=]{\mathclose{}\mathord{\nonscript\mkern0.5mu#1\nonscript\mkern0.5mu}\mathopen{}}
\newcommand*{\yF}{F}
\newcommand*{\yf}{f}
\newcommand*{\di}{\mathop{}\!\mathrm{d}}
%%
\newcommand*{\HV}{\mathit{HV}}
\newcommand*{\GDS}{\mathit{GDS}}
\newcommand*{\cAD}{\mathit{cAD}}
\newcommand*{\yy}{\mathrm{Y}}
\newcommand*{\yn}{\mathrm{N}}


\MyFilledBox{SOFTWARE HIGHLIGHT}
\begin{center}
{\large Luca\enspace Porta Mana} \\
{\tt \href{mailto:pgl@portamana.org}{pgl@portamana.org}}


\Large
\textsc{inferno: INFERence in R with Bayesian NOnparametrics}
\end{center}

\subsection*{Population inference and Bayesian nonparametrics}
\label{sec:popinference}

A very important kind of inference in research fields such as medicine is \emph{population inference}, often also called ``density inference'' or ``density regression''. Its general goal is to infer the frequency distribution of some variates in a population. This is different, for instance, from \emph{functional regression}, where the goal is to infer the functional relationship -- assumed to exist -- between a set of predictor variates and a target or ``predictand'' variate. In population inference the existence of a functional relation cannot be assumed. In fact there may not even be a clear distinction between predictor and predictand variates. A typical goal is the inference of frequency distributions within particular sub-populations or sub-groups, thus all kinds of conditional probabilities are required. A clinician may be interested in the statistics and probability of a medical condition given a symptom, but also that of a symptom given a medical condition; and maybe only within subjects of a given sex or age. De~Finetti's theorem \citep[see e.g.][\S\S\,4.2, 4.3, 4.6]{bernardoetal1994_r2000} lies at the heart of population-inference methods; a particularly brilliant discussion is given by Lindley \& Novick \citeyearpar{lindleyetal1981}.

%\citep{walker2010}

Sadly many researchers still approach population-inference problems by means of $p$-values or other frequentist practices, which only give limited, coarse, and not seldom misleading results about a population's frequency distribution. Some researchers adopt Bayesian methods but limit themselves to \emph{parametric} ones, which make very restrictive and possibly unrealistic assumptions about the population's distribution; as opposed to \emph{nonparametric} ones, which don't.

Until a couple decades ago the use of parametric methods, and maybe even of frequentist practices, was justified by pragmatic reasons. Better methods were computationally too costly or unfeasible. Population-inference problems were low-dimensional; one could \emph{visually} check whether the assumptions were appropriate to the problem and the results reasonable. Today these reasons cannot earnestly be given. Bayesian nonparametric methods have become computationally feasible for many kinds of population inference. Many population-inference problems today involve from tens to thousands of variates of different kinds; it is impossible to visually check in such high-dimensional spaces whether frequentist practices or parametric assumptions are acceptable, or by how much they err. Results may therefore be affected by large parametric-modelling errors \citep{draper1995}.

But one reason can still be given today for the avoidance of Bayesian nonparametric methods: \emph{lack of user-friendly software}. Clinicians who would be curious to try out a Bayesian nonparametric analysis of their studies simply cannot: they would need to study Markov-chain Monte Carlo techniques, programming languages to implement the latter, and a plethora of debated practices to ``assess convergence''. Most clinicians don't have time to learn all this even if they wanted to. Available packages for Bayesian nonparametrics are not quite suited to population inference. Some of them focus on functional regression, which as discussed above is not an appropriate assumption. Some make a priori distinctions between predictor and predictand variates, limiting the range of useful inferences. Most still require Monte Carlo programming expertise.

\medskip

The R-package \textbf{inferno}\footnote{\url{https://pglpm.github.io/inferno/}} was built to try to remedy the lack of this kind of software.

\subsection*{Use and features}
\label{sec:features}

The application of the package is simple. The user first provides a data sample $S$ of variates from a population, for instance the age, sex, symptom, disease, and kind of treatment of a number of patients. The package can work with any combination of continuous, discrete ordinal, discrete nominal, and binary variates. Continuous variates can also be defined in bounded intervals, and can also have boundary values capable of finite probability mass, as it may happen with censoring. The package cannot handle variates with complex topology, such as images, or periodic variates. The data sample and the variate characteristics (type, domain, possible censoring) are given by the user as two CSV files to an R function called \texttt{learn()}.

The package then does a Markov-chain Monte Carlo computation, using parallel CPUs if available, to find the probability distribution for all possible joint frequency distributions of the variates. The result is saved in an R object called \texttt{learnt}. The crucial point here is that this computation is automatic and does not require any further input from the user, who is simply informed at regular intervals about the expected end time of the computation.

Once the computation has finished, the user can inquire multiple times about any of the following:
\setlength{\leftmargini}{17.62pt}
\begin{itemize}
  \itemsep0.5ex
\item For a new unit of the population, say a new patient, the conditional probability (density) for the values of \emph{any} set of variates $A \mo a, B \mo b, \dotso$ given \emph{any} other set $C \mo c, D \mo d, \dotso$:
  \begin{equation}\label{eq:prob_var}
    \P(A \mo a, B \mo b, \dotso \|C \mo c, D \mo d, \dotso , S)
  \end{equation}
  Such a probability could for instance be used in medical decision making \citep{soxetal1988_r2024,huninketal2001_r2014}. The conditional can also be empty, and tail probabilities (e.g.\ $A \ge a$, $C \le c$) can also be requested.

\item The probability distribution for the conditional \emph{frequencies}, in the whole unsampled population, of the values of any set of variates given any other set. If we denote the frequency by $\yF$ and a specific value (distribution) by $\yf$,
  \begin{equation}\label{eq:prob_freq}
    \P\bigl[ \yF(A \mo a, \dotso \|C \mo c, \dotso) = \yf \|[\big] S\bigr]
    \,\di\yf \ .
  \end{equation}
This probability distribution is represented by Monte Carlo samples $\yf_{i}$ drawn from it.

\item The mutual information \citep[Ch.~8]{mackay1995_r2005} between any two sets of quantities.
\end{itemize}
\vspace{-\topsep}

Probabilities~\eqref{eq:prob_var} and~\eqref{eq:prob_freq} are connected by a variant of de~Finetti's theorem:
\begin{equation}
  \label{eq:definetti1}
  \P(A \mo a, \dotso \|C \mo c, \dotso , S)  =
  \int \yf \,
  \P\bigl[ \yF(A \mo a, \dotso \|C \mo c, \dotso) = \yf \|[\big] S\bigr]
  \,\di\yf
\end{equation}

In a manner of speaking, the probability distribution~\eqref{eq:prob_freq} expresses how much the probability value~\eqref{eq:prob_var} could change, if it were updated by sampling the whole population. It expresses the uncertainty in the statistical results owing to finite sample size.

The first two calculations are made by the R function \texttt{Pr(Y, X, learnt)}. The user provides a list \texttt{Y} of predictand variates and values of interest; an optional, analogous list \texttt{X} of predictors; and the \texttt{learnt} object from the Monte Carlo computation. The calculation of mutual information is made by the R function \texttt{mutualinfo(Y1names, Y2names, X)}, where the first two arguments are the variate sets of interest, and the optional third argument is a set of variate values to conditionalize upon.

\medskip

\begin{figure}[t]
\centering%
\includegraphics[width=0.45\linewidth]{figures/population_distr_HV.pdf}\hfill%
\includegraphics[width=0.45\linewidth]{figures/population_distr_GDS.pdf}%
\\ \caption{}\label{fig:distr}
\end{figure}
The package allows the user to visualize the probabilities above when just one predictand and one predictor variates are involved. If the user saves the results of the \texttt{Pr()} function in some object \texttt{probs}, say, then the visualization is done by simply calling \texttt{plot(probs)}.

Figure~\ref{fig:distr} shows two examples from a study \citep{portamanaetal2023b} about conversion from Mild Cognitive Impairment (MCI) to Alzheimer's Disease (AD); they can be used to further illustrate the probability distributions discussed above.

In the plot on the left, the predictand variate is hippocampal volume ($\HV$); the predictor is the binary, yes/no variate `will convert to AD' ($\cAD$). The solid blue thick line and dashed red thick line are the conditional probability distributions (omitting the data dependence)
\begin{equation*}
  \P( \HV \| \cAD \mo \yn)\ ,\qquad
  \P( \HV \| \cAD \mo \yy) \ .
\end{equation*}
The cloud of blue thinner lines that surrounds the first distribution above  represents the probability distribution of possible frequency distributions
\begin{equation*}
  \P\bigl[ \yF(\HV \| \cAD \mo \yn)\bigr]
\end{equation*}
each thin line is a sample from such distribution.

Looking at this plot, a clinician can immediately see that the frequency distribution of hyppocampal volume is clearly different in the sub-populations of patients that will convert to Alzheimer's and those who won't. Such a difference is almost certain even considering the uncertainty from the finite sample size.

The plot on the right is analogous but for the predictand variate `geriatric depression scale' ($\GDS$) in stead of hippocampal volume. In this case the two conditional frequency distributions cannot be distinguished within the finite-sample uncertainty.

It must be remarked that it is exactly this kind of differences and uncertainties that clinical researchers often try to clumsily capture by using $p$-values. Several practitioners expressed relief and even awe at the possibility of visualizing the estimates of different sub-population statistics and even the uncertainties they carry because of finite sample size.

The quick analysis above was purely qualitative, but concrete numbers, such as quantiles and expected values, can be given instead. This becomes necessary when many variates are considered jointly and visualization is impossible. In such high-dimensional cases the package allows the user to compute any kind of distance between two frequency distributions (such as Hellinger or Kantorovich or Shannon-Jansen distance, or relative entropy) as well as its credibility intervals. The computation of the mutual information between any two sets of variates gives moreover a quantitative measure of their association that does not depend on assumptions such as linearity or gaussianity.

\medskip

\begin{figure}[t]
\centering%
\includegraphics[width=0.45\linewidth]{figures/prob_conversion_HV.pdf}\hfill%
\includegraphics[width=0.45\linewidth]{figures/prob_conversion_GDS.pdf}%
\\ \caption{}\label{fig:distr_inv}
\end{figure}
The ability to swap the ``predictor'' and ``predictand'' roles of any variates is illustrated in the plots of fig.~\ref{fig:distr_inv}, which parallel those of fig.~\ref{fig:distr}. The thicker lines show the probabilities
\begin{equation*}
    \P( \cAD \mo \yy \| \HV)\ ,\qquad
  \P( \cAD \mo \yy \| \HV)
\end{equation*}
for various values of the $\HV$ variate, which becomes a predictor in this case. The clouds of thinner lines are samples of the corresponding probability distributions of the frequency distributions. On the plot on the left, for instance, we see that among individuals from this population having hippocampal volume around $5.0$, between 30\% and 40\% will convert to Alzheimer's. Among those with volume around $2.0$, we can only say that between roughly 45\% and 80\% will convert; in this case the finite-sample size (few samples with this $\HV$ value) leads to a much larger uncertainty of the frequency estimates.

\medskip

Plots and calculations like the ones above are of course nothing new in Bayesian nonparametrics. The point here is that the user can produce them just by first inputting the population sample in the package, waiting for the Monte Carlo computation to finish, and then simply asking about variates of interest and plotting the results. The sequence of commands could be as simple as
\begin{verbatim}
learnt <- learn(data = 'data.csv', metadata = 'metadata.csv')
## Information about MCMC and expected end time...

probs <- Pr(Y = list(A = seq(0, 8, 0.1)), X = list(C = c('yes', 'no')), learnt)

plot(probs)
\end{verbatim}

More extensive use examples are given in the package's main vignette\footnote{\url{https://pglpm.github.io/inferno/articles/inferno_start.html}}. Readers of the ISBA Bulletin are probably interested in the internals of the package, which now be briefly discussed.


\subsection*{Internals}
\label{sec:representation}

In Bayesian nonparametric population inference the posterior probability distribution is over the set of all possible frequency distributions. The mathematical representation of this set is therefore crucial. The package uses the ingenious representation of a distribution as a mixture of product kernels discussed by Dunson \& Bhattacharya \citeyear{dunsonetal2011}. For instance, for variates $A$ and $B$ the frequency distribution is written as
\begin{equation}
  \label{eq:mixture_kernel}
  \yF(A, B) = \sum_{i} w_{i}\, K_{A}(A \| \alpha_{i})\, K_{B}(B \| \beta_{i})
\end{equation}
where $w_{i}$ are positive and normalized weights, $K_{A}$ is a distribution for variate $A$ depending on parameters $\alpha_{i}$, and similarly for $K_{B}$. The product is easily generalized to any number of variates. In principle the sum should be countably infinite, but as discussed in Ishwaran \& Zarepour \citeyear{ishwaranetal2002b,ishwaranetal2002c} and in Dunson \& Bhattacharya \citeyear{dunsonetal2011}, it is possible to truncate it to a finite number of values if an appropriate prior distribution is used for the weights $w_{i}$. Thus a frequency distribution $\yF$ is effectively represented -- non-uniquely -- by a large but finite set of parameters $(w_{i}, \alpha_{i}, \beta_{i})$.

The above representation was somewhat deprecated in more recent works \citep[e.g.][]{wadeetal2014,wadeetal2014b} which, however, consider inference problems where variates have clear predictor or predictand roles. As previously discussed, in many research fields there are no such a-priori roles and any variate could be both. A representation that can easily swap the two roles and does not overemphasizing either is therefore most appropriate. The representation~\eqref{eq:mixture_kernel} leads to very simple and symmetric analytical expressions for the conditional of $A$ given $B$ and vice versa, as well as any marginal:
\begin{equation*}
  \begin{gathered}
    \yF(A \| B) = \sum_{i}
    \frac{w_{i}\, K_{A}(A \| \alpha_{i})\, K_{B}(B \| \beta_{i})}{\sum_{j} w_{j}K_{B}(B \| \beta_{j})}
\qquad\qquad    \yF(B \| A) = \sum_{i}
    \frac{w_{i}\, K_{B}(B \| \beta_{i})\, K_{A}(A \| \alpha_{i})}{\sum_{j} w_{j}K_{A}(A \| \alpha_{j})}
    \\
    \yF(A) = \sum_{i}
    w_{i}\, K_{A}(A \| \alpha_{i}) \ .
  \end{gathered}
\end{equation*}

The prior over the parameters representing the frequency distribution is a  Dirichlet-process mixture. It allows for distributions having multiple peaks, also sharp ones (with a lower probability). For information about the kernels $K$ used for the different kinds of variates, and the prior hyperdistribution see the technical manual\footnote{\url{https://github.com/pglpm/inferno/raw/main/development/manual/optimal_predictor_machine.pdf}}.

\medskip

The Markov-chain Monte Carlo computation is based on Gibbs sampling; several chains are run in parallel. The stopping rule implements some variations of the methods discussed in \cite{vehtarietal2021}. The package takes the following stand:

\textbullet\ For this kind of inference problems there is no need for tens of thousands or more independent Monte Carlo samples. The default total number of putatively-independent samples is just 3600. The mean of such samples is the probability~\eqref{eq:prob_var}, and the error in this mean is thus around $\sqrt{3600} = 60$ times smaller than the ``variability'' of this probability, given by eq.~\eqref{eq:prob_freq}.

\textbullet\ It is acceptable that the sampling has not fully converged. Consider that machine-learning algorithms such as neural networks are effectively Bayesian nonparametric functional regressors, and their training is essentially an \emph{unconverged} Monte Carlo sampling \citep{mackay1992,gal2016,mandtetal2017,huszar2017}. This ``sampling'' stops at a \emph{local} maximum of the posterior, and all samples are discarded. Thanks to this lack of full convergence the computation is fast, and despite of this lack the inferences can still be impressive. The same stance can be adopted in Bayesian nonparametrics: despite a possible lack of full convergence, the results can still be impressive and informative; and the uncertainty in the results can moreover be assessed, whereas it is discarded in neural-network training.




[[in progress]]

%\input{software.bbl}



\bibliographystyle{abbrvnat}
{\small
\bibliography{../manual/portamanabib.bib}
}

\end{document}
, that is, the inference of a functional relationship -- assumed to exist -- between a set of predictor variates and a target or ``predictand'' variate. These packages are not appropriate to \emph{population inference}, where no functional relationships exist a priori, and where often there is no a priori division between predictor and predictand variates. In clinical studies one may be interested in calculating the probability of an effect or symptom given a condition, and of a condition given an effect or symptom.
